---
layout: post
title: "VAE Tutorial 3"
subtitle: "SentenceVAE"
categories: paper
tags: dl
comments: true
---

* **VAE Tutorial 목차**
	* [Tutorial 1: CS231n 강의 내용](https://dnddnjs.github.io/paper/2018/06/19/vae/) 
	* [Tutorial 2: VAE 논문 & 코드 리뷰](https://dnddnjs.github.io/paper/2018/06/20/vae2/)
	* [Tutorial 3: SentenceVAE](https://dnddnjs.github.io/paper/2018/06/21/vae3/)
	* [Tutorial 4: MusicVAE](https://dnddnjs.github.io/paper/2018/06/21/vae4/)



# VAE Tutorial 3: SequenceVAE

<img src="https://www.dropbox.com/s/hz7whkd5iv9n203/Screenshot%202018-06-21%2023.41.18.png?dl=1">

- 논문 저자: Samuel R. Bowman (Stanford NLP group)
- 논문 링크: [https://arxiv.org/pdf/1511.06349.pdf](https://arxiv.org/pdf/1511.06349.pdf)
- 참고한 자료: 
	- [https://arxiv.org/pdf/1803.05428.pdf](https://arxiv.org/pdf/1803.05428.pdf)

---

## 1. Introduction
[MusicVAE 논문](https://arxiv.org/pdf/1803.05428.pdf)의 introduction 내용을 참고했습니다. 

Tutorial 1과 2에서 다룬 VAE는 generative model입니다. GAN과는 달리 모델의 explicit하게 학습할 수 있는 explicit density model 입니다. 또한 같은 explicit density model 중의 하나인 PixelCNN & PixelRNN 과는 다르게 데이터의 의미있는 latent representation을 학습할 수 있습니다. 이 때 model은 Deep Neural Network를 사용해서 복잡한 transformation을 표현하도록 합니다. 따라서 VAE를 다른 이름으로는 deep latent variable model이라고도 부릅니다. 

VAE는 이전 논문과 코드에서 살펴봤듯이 image를 생성하는데 초첨이 맞춰서 발전했습니다. VAE를 사용해서 여러 흥미로운 연구들이 이뤄졌습니다. 대부분은 VAE가 latent code를 학습하기 때문에 할 수 있는 것들입니다. 예를 들어 갈색 머리 사람의 사진을 모아서 encoder를 통과해 나온 latent vector를 평균 취하면 갈색 머리에 대한 "attribute vector"를 구할 수 있습니다. 만약 검은색 머리에 대한 attribute vector도 구했다면 재밌는 것을 할 수 있습니다. 다음 그림은 "Autoencoding beyond pixels using a learned similarity metric"이라는 논문에서 가져왔는데 attribute vector를 통해 input data를 변형시켜 새로운 data를 생성하는 것을 보여줍니다. 머리색이 변하기도 하고 성별이 변하기도 하며 수염이 생기기도 합니다. 

<img src="https://www.dropbox.com/s/5l4x0xu1kykpavw/Screenshot%202018-06-21%2014.43.53.png?dl=1">
<center>그림 출처 https://arxiv.org/pdf/1512.09300v1.pdf </center>

<br>

또한 latent space 상에서 서로 다른 두 latent vector의 interporlation을 통해 재밌는 실험을 할 수도 있습니다. 다음 그림은 "Deep Feature Consistent Variational Autoencoder" 논문에서 가져왔습니다. 그림의 왼쪽 사진에 대한 latent vector를 얻어내고 오른쪽 사진에 대한 latent vector를 얻어내면 $$z_{new} = \alpha z_{left} + (1-\alpha) z_{right}$$와 같은 식으로 linear interpolation으로 새로운 latent vector를 구할 수 있습니다. 다음 그림은 이 $$\alpha$$ 값을 0에서 1사이에서 변형시켜가며 그 latent vector를 decoder 통과시켰을 때 나온 이미지를 보여준 것입니다. continuous 하게 왼쪽 이미지에서 오른쪽 이미지로 변해가는 것을 볼 수 있습니다. 

<img src="https://www.dropbox.com/s/gp7xncdtri29ox7/Screenshot%202018-06-21%2015.21.02.png?dl=1">
<center>그림 출처 https://arxiv.org/pdf/1610.00291.pdf</center>

<br>
이러한 연구들은 대부분 이미지 데이터에 대해서 진행되었습니다. 하지만 최근 이와는 다르게 sequential data를 generation 하는 VAE에 대한 연구들도 이뤄지고 있습니다. Sequential한 data를 생성하기 위해서는 latent vector를 통해 data를 generate하는 decoder가 그냥 CNN이나 MLP가 아닌 RNN이나 PixelRNN 같은 autoregressive model을 사용해야 합니다(latent vector가 RNN에 대한 attention과 같이 작동한다고 생각하시면 이해가 쉬울지도 모르겠습니다 혹은 condition). 문제는 VAE의 decoder에 autoregressive model를 사용할 경우 latent code를 무시해버린다는 것입니다. 이러한 현상은 더 long sequence data를 생성할 때 더 명확히 드러납니다. decoder가 latent code를 무시하는 현상을 "posterior collapse"라고 합니다. 이 논문에서는 "posterior collapse" 문제를 해결하기 위해 ELBO의 KL term에 weight를 곱해주는 방법과 RNN(decoder)의 condition을 조정하는 방법을 사용했습니다. 


sentence에 대해서도 image에서와 마찬가지로 latent vector interpolation을 테스트해볼 수 있습니다. 아래 사진은 "Generating Sentences from a Continuous Space" 논문에서 가져온 사진입니다. "i went to the store to buy some groceries ."라는 문장의 latent vector와 "horses are my favorite animal ."라는 문장의 latent vector 사이를 linear 하게 interpolation 한 것입니다. 문장이 조금씩 변해가는 것을 볼 수 있습니다. 학습한 latent code는 입력 문장의 스타일, 주제, 의미 등을 담아냅니다. 문장을 생성하도록 학습한 모델을 SentenceVAE라고 합니다. 이제 살펴보도록 하겠습니다.

<center><img src="https://www.dropbox.com/s/p8xoq9oq9edgg4m/Screenshot%202018-06-21%2018.11.17.png?dl=1" width="400px"></center>
<center>그림 출처 https://arxiv.org/pdf/1511.06349.pdf</center>

<br>

---
## 2. A VAE for sentences
(논문이 쓰여질 당시에)기존에 문장을 생성해내는 generative model 중에 state-of-art는 RNNLM(Recurrent Neural Network Language Model)이었습니다. Machine Translation, Image captioning과 같은 task에서 두각을 드러냈지만 문장의 global한 feature를 학습하지 못한다는 단점이 있었습니다. 따라서 이 논문은 VAE라는 새로운 네트워크를 통해 latent vector가 문장의 global feature를 나타내게 하겠다는 것이 목표입니다. RNNLM과 VAE를 적절히 합친다는 뜻입니다. VAE가 hidden latent variable로부터 data를 생성해내는데 여러 task에서 이미 보였듯이 의미있는 representation을 학습합니다. 따라서 RNNLM의 문제를 해결하는데 적절한 방법으로 보이는 것입니다. 다음은 이 논문에서 VAE를 설명하는 부분입니다. 이와같이 만들어낸 사람이 아닌 다른 사람의 해석을 보는 것이 직관력을 기르는데 도움이 됩니다. 

> The variational autoencoder (vae, Kingma and
Welling, 2015; Rezende et al., 2014) is a generative
model that is based on a regularized version
of the standard autoencoder. This model imposes
a prior distribution on the hidden codes z which
enforces a regular geometry over codes and makes
it possible to draw proper samples from the model
using ancestral sampling.
The vae modifies the autoencoder architecture
by replacing the deterministic function ϕenc with
a learned posterior recognition model, q(z|x). This
model parametrizes an approximate posterior distribution
over z (usually a diagonal Gaussian) with
a neural network conditioned on x. Intuitively, the
vae learns codes not as single points, but as soft
ellipsoidal regions in latent space, forcing the codes
to fill the space rather than memorizing the training
data as isolated codes.

VAE의 학습 목표인 ELBO는 다음과 같습니다. 이 때, $$g_{\phi}(\epsilon, x) = \mu + \sigma\epsilon$$ 입니다(reparametization trick).
 
$$\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))$$

VAE를 sequence generation에 사용하기 위해 encoder와 decoder에 MLP가 아닌 single layer LSTM을 사용합니다. prior는 똑같이 gaussian 분포를 사용합니다. 이 gaussian prior는 ELBO의 KL term에 의해서 일종의 regularizer로 작용합니다. 그리고 latent code는 decoder에 condition으로 작용하며 generation 과정을 조절합니다. 이 모델의 구조는 다음과 같습니다. 문장은 단어 단위로 LSTM에 input으로 들어갑니다. 이 input은 word embedding의 output vector 입니다. 문장이 모두 끝까지 입력으로 들어가고 나면 hidden state의 값이 latent space 상에서 multivariate gaussian의 mean과 variance가 됩니다. 그러면 reparameterization trick을 사용해서 latent code z를 sampling 할 수 있습니다. 이 latent code가 decoder LSTM 모델의 initial hidden state가 되어 word space 상에서의 확률을 output으로 내보냅니다. 

<img src="https://www.dropbox.com/s/6islhw0mneo3to5/Screenshot%202018-06-22%2000.32.45.png?dl=1">

<br>


<br>

---
## 3. Experiment & Conclusion
### 3.1 Experiment

### 3.2 Conclusion

<br>

---
## 4. Code Review
 
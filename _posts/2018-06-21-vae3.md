---
layout: post
title: "VAE Tutorial 3"
subtitle: "VAE 적용 MusicVAE"
categories: paper
tags: dl
comments: true
---

* **VAE Tutorial 목차**
	* [Tutorial 1: CS231n 강의 내용](https://dnddnjs.github.io/paper/2018/06/19/vae/) 
	* [Tutorial 2: VAE 논문 & 코드 리뷰](https://dnddnjs.github.io/paper/2018/06/20/vae2/)
	* [Tutorial 3: VAE 적용 MusicVAE](https://dnddnjs.github.io/paper/2018/06/21/vae3/)



# VAE Tutorial 3: VAE 적용 MusicVAE

<img src="https://www.dropbox.com/s/b4pqdgaluovew27/Screenshot%202018-06-21%2014.09.55.png?dl=1">

- 논문 저자: Adam Roberts (Google Brain)
- 논문 링크: [https://arxiv.org/pdf/1803.05428.pdf](https://arxiv.org/pdf/1803.05428.pdf)
- 참고한 자료: 
	- [https://magenta.tensorflow.org/music-vae](https://magenta.tensorflow.org/music-vae)

이 논문을 Tutorial에 넣은 이유는 다음과 같은 이유에서입니다.

1. VAE의 발전과 흐름에 대해서 잘 설명한다.
2. image가 아닌 sequential data generation 이다.
3. 이미 웹에 application이 프로토타입으로 구현되어 올라와있다.

다음 유투브 링크는 MusicVAE에 대해서 설명해주는 동영상입니다. 안봤다면 한 번 보는 것을 추천합니다. 한 문장으로 요약하자면 "음악데이터에 대해 VAE를 학습한 다음에 학습한 latent code를 가지고 이러저러한 음악 창작 도구로 사용할 수 있다"라고 볼 수 있습니다. 밑의 화면을 **클릭** 해주세요.

[![musicvae youtube](https://img.youtube.com/vi/G5JT16flZwM/0.jpg)](https://youtu.be/G5JT16flZwM)

<br>

---

## 1. Introduction
Tutorial 1과 2에서 다룬 VAE는 generative model입니다. GAN과는 달리 모델의 explicit하게 학습할 수 있는 explicit density model 입니다. 또한 같은 explicit density model 중의 하나인 PixelCNN & PixelRNN 과는 다르게 데이터의 의미있는 latent representation을 학습할 수 있습니다. 이 때 model은 Deep Neural Network를 사용해서 복잡한 transformation을 표현하도록 합니다. 따라서 VAE를 다른 이름으로는 deep latent variable model이라고도 부릅니다. 

VAE는 이전 논문과 코드에서 살펴봤듯이 image를 생성하는데 초첨이 맞춰서 발전했습니다. VAE를 사용해서 여러 흥미로운 연구들이 이뤄졌습니다. 대부분은 VAE가 latent code를 학습하기 때문에 할 수 있는 것들입니다. 예를 들어 갈색 머리 사람의 사진을 모아서 encoder를 통과해 나온 latent vector를 평균 취하면 갈색 머리에 대한 "attribute vector"를 구할 수 있습니다. 만약 검은색 머리에 대한 attribute vector도 구했다면 재밌는 것을 할 수 있습니다. 다음 그림은 "Autoencoding beyond pixels using a learned similarity metric"이라는 논문에서 가져왔는데 attribute vector를 통해 input data를 변형시켜 새로운 data를 생성하는 것을 보여줍니다. 머리색이 변하기도 하고 성별이 변하기도 하며 수염이 생기기도 합니다. 

<img src="https://www.dropbox.com/s/5l4x0xu1kykpavw/Screenshot%202018-06-21%2014.43.53.png?dl=1">
<center>그림 출처 https://arxiv.org/pdf/1512.09300v1.pdf </center>

<br>

또한 latent space 상에서 서로 다른 두 latent vector의 interporlation을 통해 재밌는 실험을 할 수도 있습니다. 다음 그림은 "Deep Feature Consistent Variational Autoencoder" 논문에서 가져왔습니다. 그림의 왼쪽 사진에 대한 latent vector를 얻어내고 오른쪽 사진에 대한 latent vector를 얻어내면 $$z_{new} = \alpha z_{left} + (1-\alpha) z_{right}$$와 같은 식으로 linear interpolation으로 새로운 latent vector를 구할 수 있습니다. 다음 그림은 이 $$\alpha$$ 값을 0에서 1사이에서 변형시켜가며 그 latent vector를 decoder 통과시켰을 때 나온 이미지를 보여준 것입니다. continuous 하게 왼쪽 이미지에서 오른쪽 이미지로 변해가는 것을 볼 수 있습니다. 

<img src="https://www.dropbox.com/s/gp7xncdtri29ox7/Screenshot%202018-06-21%2015.21.02.png?dl=1">
<center>그림 출처 https://arxiv.org/pdf/1610.00291.pdf</center>

<br>
이러한 연구들은 대부분 이미지 데이터에 대해서 진행되었습니다. 하지만 최근 이와는 다르게 sequential data를 generation 하는 VAE에 대한 연구들도 이뤄지고 있습니다. Sequential한 data를 생성하기 위해서는 latent vector를 통해 data를 generate하는 decoder가 그냥 CNN이나 MLP가 아닌 RNN이나 PixelRNN 같은 autoregressive model을 사용해야 합니다(latent vector가 RNN에 대한 attention과 같이 작동한다고 생각하시면 이해가 쉬울지도 모르겠습니다 혹은 condition). 문제는 VAE의 decoder에 autoregressive model를 사용할 경우 latent code를 무시해버린다는 것입니다. 이러한 현상은 더 long-sequence data를 생성할 때 더 명확히 드러납니다. decoder가 latent code를 무시하는 현상을 "posterior collapse"라고 합니다. 이 문제에 대해서는 "Generating Sentences from a Continuous Space" 논문에서 잘 설명해주고 있습니다. 이 논문에서는 "posterior collapse" 문제를 해결하기 위해 ELBO의 KL term에 weight를 곱해주는 방법과 RNN(decoder)의 condition을 조정하는 방법을 사용했습니다. 


sentence에 대해서도 image에서와 마찬가지로 latent vector interpolation을 테스트해볼 수 있습니다. 아래 사진은 "Generating Sentences from a Continuous Space" 논문에서 가져온 사진입니다. "i went to the store to buy some groceries ."라는 문장의 latent vector와 "horses are my favorite animal ."라는 문장의 latent vector 사이를 linear 하게 interpolation 한 것입니다. 문장이 조금씩 변해가는 것을 볼 수 있습니다. 

<center><img src="https://www.dropbox.com/s/p8xoq9oq9edgg4m/Screenshot%202018-06-21%2018.11.17.png?dl=1" width="400px"></center>
<center>그림 출처 https://arxiv.org/pdf/1511.06349.pdf</center>





 
---
layout: post
title: "TRUST-PCL: An Off-policy Trust Region Method for Continuous Control"
subtitle: "TRPO를 off-policy 가능하게!"
categories: paper
tags: rl
comments: true
---

# TRUST-PCL: An Off-policy Trust Region Method for Continuous Control [2018]

<img src="https://www.dropbox.com/s/nubod5d36dpy6yd/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2011.01.01.png?dl=1">

- 논문 저자: Ofir Nachum(Google Brain)
- 논문 링크: [https://openreview.net/pdf?id=HyrCWeWCb](https://openreview.net/pdf?id=HyrCWeWCb)
- 함께 보면 좋을 논문: 
	- [Trust Region Policy Optimization (2015)](https://arxiv.org/pdf/1502.05477.pdf)
	- [Bridging the Gap Between Value and Policy Based Reinforcement Learning(2017)](https://arxiv.org/abs/1702.08892)
- 논문을 보는 이유: TRPO의 어떤 문제가 있고 그걸 어떤 방법으로 해결하고자 했는지 궁금해서

## 1. Abstract
---

- Trust region 최적화 방법은 policy optimization 과정을 안정화하기 위해서 자주 사용된다.
- 하지만 TRPO와 같은 Trust Region을 활용한 알고리즘들은 on-policy 알고리즘이기 때문에 학습 데이터가 많이 필요하다.
- 이 문제를 해결하기 위해서 TRPO + PCL인 Trust-PCL 알고리즘을 제안한다.
- maximum reward objective에 relative-entropy regularizer를 더한 새로운 objective의 optimal policy와 state value가 multi-step pathwise consistency를 만족한다.
- relative entropy regularization은 off-policy data로부터 학습하면서도 최적화 과정이 안정적이게 해준다.
- 여러 continuous control 문제에서 TRPO보다 Trust-PCL이 solution의 quality와 sample efficiency가 더 좋다.

[개인생각]
기존의 PPO 등이 있지만 현재 continuous control 문제에서 성능만 놓고 봤을 때 state-of-art는 TRPO이다. 이 TRPO는 on-policy로 현재 policy로 모은 데이터로만 학습을 하기 때문에 데이터가 많이 필요하다는 단점이 있다. 따라서 저자가 이전에 작성한 "Bridging the Gap Between Value and Policy Based Reinforcement Learning"논문에서 소개했던 PCL을 TRPO와 잘 융합해서 off-policy가 가능하도록 만들겠다는 것이 논문의 내용이다. 

PCL의 논문 내용을 알아야하기 때문에 이 논문을 그냥 보고 이해하는 것은 어려울 것 같다. 논문에서 PCL + TRPO에 대해서는 잘 설명하다가 off-policy가 가능한 이유에 대해서는 그닥 잘 설명하는 느낌을 못 받았다. 아마 PCL 논문에 나와있을 것 같다(PCL 자체가 on-policy로 모은 데이터로도 학습할 수 있고 off-policy로 모은 데이터로도 학습할 수 있다고 한다. 

논문이 제시하는 방법은 잘 이해못하더라도 논문의 related work 부분에서 TRPO에 대해서 정리해주는 점은 참고할만 한 것 같다. 

## 2. Conclusion
---

- Trust-PCL을 제안한다. 한 문장으로 표현하자면 다음과 같다. "an off-policy algorithm employing a relative-entropy penalty to impose
a trust region on a maximum reward objective" (핵심은 "off-policy" "relative-entropy penalty" "trust region" 이라고 볼 수 있을 듯)
- TRPO보다 Trust-PCL이 average reward도 더 좋고 sample-efficiency도 더 좋다.
- 그러므로 stability와 sample efficiency 두 마리 토끼를 다 잡았다.


## 3. Experiment
---
- 테스트 환경: Acrobot, HalfCheetah, Swimmer, Hopper, Walker2d, and Ant
- TRPO는 한 번 업데이트하기 위해 25,000 개의 sample을 모았다.
- Trust-PCL은 off-policy 이므로 sample을 모으는 것과 학습을 번갈아가면서 했다. 10 step 마다 한 번씩 64개의 mini-batch를 replay memory에서 추출해서 학습했다.
- 이 때, replay memory에서 더 최근에 더한 sample일수록 priority를 줬다. 
- 다음 그래프에서 보듯이 Trust-PCL은 TRPO보다 점수도 더 높고 더 빠르게 학습하는 경향이 있다. 

<img src="https://www.dropbox.com/s/9vtr0k4vbyo5ct1/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2011.49.24.png?dl=1">

- 다음 그래프는 on-policy Trust-PCL와 off-policy Trust-PCL의 차이를 보여준다.

<img src="https://www.dropbox.com/s/l7zzu19shiv3jc5/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2011.57.25.png?dl=1">

- 다음 표는 여러 알고리즘의 최종 성능을 비교한다. 10M step을 학습한 결과이다. Trust-PCL이 Walker 빼고 다 점수가 제일 높다.

<img src="https://www.dropbox.com/s/faolncuwyza76uj/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2011.59.36.png?dl=1">

## 4. Related Work
---
일단 gradient descent에 대해서 다음과 같이 정의한다. natural policy gradient 논문을 읽었다면 알겠지만 이것은 Euclidean space 일 때 성립하는 식이다. 한 가지 주목할 점은 그냥 gradient descent 또한 trust region으로 constraint가 주어진 optimization으로 본다는 것이다.
<img src="https://www.dropbox.com/s/lh9n44gpwncpi50/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2012.01.17.png?dl=1"> 

보통 뉴럴넷을 사용할 경우 Euclidean geometry 보다는 Riemannian geometry가 더 적합하다. 이 경우에 loss surface에 대해서 Riemannian metric를 정의하는 것이 좋은데 이 metric을 통해 local에서의 steep descent direction을 구할 수 있다. Natural gradient를 이용할 경우 이 metric은 Fisher information matrix가 된다. FIM은 conditional probability model의 log-likelihood를 최적화할 때 유용하다 ($$\nabla log \pi_{\theta}$$의 형태를 떠올려보면 무슨 말인지 대충 느낌이 올 것이다). 다음은 Natural Policy Gradient에 대한 내용이다. Bregman divergence는 처음 듣는다. 

<img src="https://www.dropbox.com/s/zx6jfob1235brl2/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2012.08.43.png?dl=1"> 

결국 다음 식을 보면 natural policy gradient에서 하고 있는 것은 KL-divergence를 fisher information matrix를 통해 approximation하고 있는 것이다.
<img src="https://www.dropbox.com/s/66td1itm57s3lqv/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2012.10.15.png?dl=1">

TRPO는 natural gradient에 몇 개의 approximation을 더해서 nonlinear policy optimization을 할 수 있도록 만들었다.

---
이 이외의 내용은 PCL에 대한 이해를 필요로 한다. 후에 만약 PCL 논문을 읽게 되면 다시 읽어보면 좋을 것 같다.
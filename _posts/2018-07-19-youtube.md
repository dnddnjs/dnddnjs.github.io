---
layout: post
title: "Playing hard exploration games by watching YouTube"
subtitle: "유투브로 배우는 에이전트"
categories: paper
tags: rl
comments: true
---

## 논문 제목: Playing hard exploration games by watching YouTube [2018 March]

<img src="https://www.dropbox.com/s/p8r1y3xxeurh8ko/Screenshot%202018-07-19%2022.33.07.png?dl=1">

- 논문 저자: Yusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang, Nando de Freitas (DeepMind)
- 논문 링크: [https://arxiv.org/pdf/1805.11592.pdf](https://arxiv.org/pdf/1805.11592.pdf)
- 유투브 링크: [https://youtu.be/Msy82sIfprI](https://youtu.be/Msy82sIfprI)

<br>

## 1. Abstract
---

- 이 논문은 sparse reward인 문제 상황을 지적
- 이런 문제 상황에서 exploration을 잘하는 것이 중요함
- 이전에는 사람이 reference가 될 수 있는 demonstation을 만들고 에이전트가 모방하도록 하는 방법이 있었음
- 하지만 demonstration을 얻는 것은 여러가지 이슈로 쉽지 않음
- 이 논문에서는 noisy하고 unaligned되어있는 video로부터 학습하는 방법을 제시함. 이 방법은 2 stage로 나뉨.
- 우선 self-supervised learning을 통해 여러개의 유투브 영상에서 common representation을 학습
- 그 다음 하나의 유투브 영상을 embed해서 reward를 만들고 그걸로 agent를 학습시킴
- 이렇게 학습하니 human-level performance 이상을 낼 수 있었음
- 테스트는 Montezuma's revenge외 2개의 아타리 게임에 대해서 함.

이 논문은 유투브 영상으로부터 그 유명한 몬테주마 리벤지 게임을 학습하는 논문이다. 준비물은 몬테주마 리벤지를 플레이하는 유투브 영상 몇 개이다. 이 논문은 크게 다음 세 가지를 알면 된다.
 
1. 어떻게 유투브 영상에서 representation을 학습해내는지
2. 학습한 representation을 가지고 어떻게 reward를 만들어내는지
3. 어떻게 학습한 embedding을 평가하는지

<br>

## 2. Experiment Result
---
이 논문에서 제시하는 방법을 썼을 경우 사람보다 더 잘 플레이할 수 있다. 아래 표가 그 결과를 보여준다. 기존 Rainbow 등과 차이가 많이 난다.
<img src="https://www.dropbox.com/s/ngelr1n3ep8hpqh/Screenshot%202018-07-26%2022.57.22.png?dl=1">

다음 그림은 학습 곡선이다. 기존 강화학습만을 사용한 알고리즘은 거의 학습이 안되는 반면 논문에서 제시하는 방법은 학습이 잘 되는 것을 볼 수 있다. 
<img src="https://www.dropbox.com/s/8m70cbbi6m03xyl/Screenshot%202018-07-26%2022.58.58.png?dl=1">

이러한 결과를 봤을 때 기존 강화학습과는 확실한 차이가 있음을 알 수 있다. 사실 몬테주마 리벤지를 푸는 방법은 여러가지가 있어서 이 논문이 이 문제를 '처음' 푼 것은 아니다. 하지만 유투브 영상 몇 개만으로 사람의 실력을 넘어서는건 상당히 의미가 있다고 본다. 유투브로부터 에이전트가 학습하는 법을 배울 수 있을 경우 앞으로 무엇이 가능할지 상상해보라.

<br>

## 3. Closing the domain gap
---

이 논문이 하고 싶은 것은 다음 그림과 같다. 왼쪽이 실제 에이전트가 있는 게임화면이고 나머지가 유투브 영상이다. 이 유투브 영상들을 통해 에이전트가 게임 플레이하는 법을 학습할 것이다. 하지만 문제는 유투브 영상끼리 frame이 맞지 않고 align도 안 맞다는 점이다. 또한 밑에 그림에서 볼 수 있듯이 유투브 영상끼리도 색상이 다르다. 
<img src="https://www.dropbox.com/s/rr3wrmhuuijnfmc/Screenshot%202018-07-26%2023.08.06.png?dl=1">

따라서 이 논문에서는 이 영상들로부터 바로 에이전트를 학습시키는게 아니라 단계를 나눈다. 일단 영상들로부터 어떠한 공통적인 representation을 학습한다. 이 때 사용하는 방법을 TDC, CMC라고 부른다. TDC는 Temporal distance classification으로 영상에서 frame 끼리 얼마나 시간적 차이가 있는지 예측하는 것을 통해 representation을 학습하는 방법이다. CMC는 Cross-modal temporal distance classification으로 유투브 영상에 포함되어있는 소리 정보를 통해 TDC와 비슷하게 소리끼리의 시간 차이를 예측하는 것을 통해 representation을 학습하는 방법이다. representation을 학습하면 그 정보를 가지고 imitation을 위한 reward를 생성한다. 이제 어떻게 representation을 학습하는지 살펴보자.

### 3.1 TDC
다음 그림으로 살펴보는 것이 이해하기 가장 좋다. 
<img src="https://www.dropbox.com/s/117n8s3ruva0y40/Screenshot%202018-07-26%2023.23.09.png?dl=1">

위 그림에서 초록색 부분이 TDC이다. video가 있으면 그 video는 여러 frame으로 구성되어있을 것이다. 그 frame들 사이의 시간적 차이를 prediction 하는 unsupervised task로 학습하는 것이다. 이 과정에서 모델을 의미있는 representation을 학습하게 된다. 

시간은 discrete하게 몇 개의 구간으로 나누는데 논문에서는 6개의 구간으로 나누었다. 
<img src="https://www.dropbox.com/s/w5oezcrlelh5u64/Screenshot%202018-07-26%2023.25.51.png?dl=1">

즉, 두 개의 image가 주어지면 모델은 그 두 개를 입력받아서 6개의 category 중 하나로 classify하는 것이다. v와 w가 각각 phi라는 일반적인 CNN 모델을 통해 feature로 변환이 되면 이 feature 들 간에 element-wise multiplication을 한다. 그리고 MLP를 통과해서 6개의 output 노드를 통해 각 category일 확률을 output으로 내보낸다. 결과를 가지고 다음과 같이 cross entropy loss를 만든다. y는 label이고 y^은 prediction이다. 

<img src="https://www.dropbox.com/s/cxtisa50sav4scu/Screenshot%202018-07-26%2023.30.54.png?dl=1">

visual embedding 모델과 classifier 모델의 세부사항은 다음과 같다. 모델 부분은 크게 특별한 것은 없다.

<img src="https://www.dropbox.com/s/ld7cgjvysk2lc53/Screenshot%202018-07-26%2023.32.37.png?dl=1">
<img src="https://www.dropbox.com/s/68wk22d2ur9odbj/Screenshot%202018-07-26%2023.33.21.png?dl=1">

### 3.2 CMC
유투브 영상에서 소리는 의미있는 정보를 포함하고 있다. 특히 아타리 게임과 같은 게임의 경우 소리는 어떤 event의 발생을 의미하는 경우가 많다. 따라서 video frame 뿐만 아니라 소리에 대해서도 temporal distance classification을 수행한다. 다시 아래 그림을 살펴보자. 소리에 대해서 classification 할 때는 소리끼리 하는 것은 아니고 frame 하나 소리 하나가 주어지고 두 개 사이의 관계를 알아내는 것이다. 이 경우 category가 2개라서 모델은 소리가 해당 frame과 같이 일어나는 것인지 아닌지만 classify하면 된다. 
<img src="https://www.dropbox.com/s/117n8s3ruva0y40/Screenshot%202018-07-26%2023.23.09.png?dl=1">

CMC의 모델에 대한 상세한 정보는 다음과 같다. stft + conv1d 라고 볼 수 있다.

<img src="https://www.dropbox.com/s/6pf66nz9d0b3hhb/Screenshot%202018-07-26%2023.57.12.png?dl=1">

학습할 때는 TDC loss와 CMC loss를 weight를 곱해서 더해서 하나의 loss로 만들어서 학습한다. 

### 3.3 Cycle-consistency
이렇게 학습한 embedding이 얼마나 좋은 것인지를 판단하는 것이 어렵다. 이 논문에서는 CycleGAN에서 사용되었던 'cycle relation' 개념을 활용한다. 만약 두 개의 sequence V = {v1, v2, v3, ...}와 W = {w1, w2, w3, ...}가 있다고 하면 이 두 개의 sequence에 대해 다 embedding을 구할 수 있다. 이 때, 각 embedding에 대해 Euclidean distance를 구할 수 있다. 
<img src="https://www.dropbox.com/s/tqt3pqerfm2rl3n/Screenshot%202018-07-26%2023.42.36.png?dl=1">

이 때 특정 vi에 대해서 W 중에서 embedding 상에서 가장 거리가 짧은 것을 찾았더니 wj였다면 그 반대도 성립하는 것을 cycle consistency라고 한다. cycle consistency를 만족할 확률을 one-to-one alignment capacity라고 하며 이것을 통해 embedding이 잘 학습되었는지를 파악할 수 있다. 여러가지 embedding 모델들의 cycle consistency를 비교한 것은 다음과 같다. 역시 논문에서 제시한 방법이 one-to-one alignment capacity가 가장 높다. 
<img src="https://www.dropbox.com/s/ksu4ekly1qxtt8c/Screenshot%202018-07-26%2023.45.42.png?dl=1">

또한 embedding space를 시각화해볼 수 있는데 논문에서는 t-SNE를 가지고 시각화를 하였다. 색깔이 다른 것이 서로 다른 유투브 영상이라는 뜻이다. pixel만 가지고 embedding을 할 경우 완전 실패하고 TDC만 사용할 경우에 여러가지 영상끼리 align이 잘 안 맞는다. TDC에다 CMC까지 사용할 경우 아래 그림의 맨 오른쪽 처럼 서로 다른 색깔이 비슷하게 가는 것을 볼 수 있다. 맨 위에 첨부한 유투브 영상에 이와 관련된 부분이 있으니 찾아보길 바란다. 
<img src="https://www.dropbox.com/s/g1l52e890mlbkw4/Screenshot%202018-07-26%2023.47.43.png?dl=1">


<br>
## 4. One-shot imitation from Youtube footage
이렇게 representation을 학습했다면 이제 이 representation을 통해 agent를 학습시키는 단계이다. 에이전트를 학습시킬 때는 유투브 영상 하나만 사용한다. 유투브 영상을 frame 마다 embedding을 하면 trajectory가 생긴다. 이 trajectory에서 16 frame 마다를 checkpoint로 정한다. 그 checkpoint 마다 다음 수식을 통해 reward를 정의한다. 
<img src="https://www.dropbox.com/s/cb140iezg3hw765/Screenshot%202018-07-26%2023.53.59.png?dl=1">

학습에 사용되는 agent는 IMPALA를 사용했다. 여기서 약간 허탈하지만 (actor가 100개나 된다...) 충분히 의미는 있다고 생각한다. 학습하고 나서 embedding model의 마지막 convolution layer를 한 번 visualize 해봤다. 각 뉴런이 상당히 의미있는 부분들을 포착하는 것을 볼 수 있고 CMC가 좀 더 key item에 집중하는 것을 볼 수 있다. 이것을 통해 embedding이 잘 학습된 것을 볼 수 있다.

<img src="https://www.dropbox.com/s/jufq5uj53snqebh/Screenshot%202018-07-26%2023.59.22.png?dl=1">


## 5. 개인생각
결국 몬테주마 리벤지 게임을 학습하기 위한 새로운 reward를 설계하는데 사람이 하지 않고 유투브 영상으로부터 배운 representation으로 하겠다는게 논문의 핵심이다. 하지만 이 논문에서와 다르게 영상에서 소리가 의미가 없는 경우도 많다. 또한 이 경우는 시간과 색상의 alignment가 영상과 실제 환경이 다른 것이지 화면 자체는 동일하다고 볼 수 있다. 하지만 많은 경우 에이전트가 유투브 영상으로부터 배울 때 이러한 조건들이 만족하지 않을 수 있다. 그럴 경우 어떻게 유용한 embedding을 학습하고 그로부터 reward를 잘 만들어낼지는 좀 더 살펴봐야할 것 같다.

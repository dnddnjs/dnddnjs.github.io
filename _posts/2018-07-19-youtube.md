---
layout: post
title: "Playing hard exploration games by watching YouTube"
subtitle: "유투브로 배우는 에이전트"
categories: paper
tags: rl
comments: true
---

<img src="https://www.dropbox.com/s/p8r1y3xxeurh8ko/Screenshot%202018-07-19%2022.33.07.png?dl=1">
- 논문 저자: Yusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang, Nando de Freitas (DeepMind)
- 논문 링크: [https://arxiv.org/pdf/1805.11592.pdf](https://arxiv.org/pdf/1805.11592.pdf)
- 유투브 링크: [https://youtu.be/Msy82sIfprI](https://youtu.be/Msy82sIfprI)

<br>

## 1. Abstract
---

- 이 논문은 sparse reward인 문제 상황을 지적. 이런 문제 상황에서 exploration을 잘하는 것이 중요함
- 이전에는 사람이 reference가 될 수 있는 demonstation을 만들고 에이전트가 모방하도록 하는 방법이 있었음
- 하지만 demonstration을 얻는 것은 여러가지 이슈로 쉽지 않음
- 이 논문에서는 noisy하고 unaligned되어있는 video로부터 학습하는 방법을 제시함. 이 방법은 2 stage로 나뉨.
- 우선 self-supervised learning을 통해 여러개의 유투브 영상에서 common representation을 학습
- 그 다음 하나의 유투브 영상을 embed해서 reward를 만들고 그걸로 agent를 학습시킴
- 이렇게 학습하니 human-level performance 이상을 낼 수 있었음
- 테스트는 Montezuma's revenge외 2개의 아타리 게임에 대해서 함.

이 논문은 유투브 영상으로부터 그 유명한 몬테주마 리벤지 게임을 학습하는 논문이다. 준비물은 몬테주마 리벤지를 플레이하는 유투브 영상 몇 개이다. 이 논문은 크게 다음 세 가지를 알면 된다.
 
1. 어떻게 유투브 영상에서 representation을 학습해내는지
2. 학습한 representation을 가지고 어떻게 reward를 만들어내는지
3. 어떻게 학습한 embedding을 평가하는지

<br>

## 2. Experiment Result
---
이 논문에서 제시하는 방법을 썼을 경우 사람보다 더 잘 플레이할 수 있다. 아래 표가 그 결과를 보여준다. 기존 Rainbow 등과 차이가 많이 난다.
<img src="https://www.dropbox.com/s/ngelr1n3ep8hpqh/Screenshot%202018-07-26%2022.57.22.png?dl=1">

다음 그림은 학습 곡선이다. 기존 강화학습만을 사용한 알고리즘은 거의 학습이 안되는 반면 논문에서 제시하는 방법은 학습이 잘 되는 것을 볼 수 있다. 
<img src="https://www.dropbox.com/s/8m70cbbi6m03xyl/Screenshot%202018-07-26%2022.58.58.png?dl=1">

이러한 결과를 봤을 때 기존 강화학습과는 확실한 차이가 있음을 알 수 있다. 사실 몬테주마 리벤지를 푸는 방법은 여러가지가 있어서 이 논문이 이 문제를 '처음' 푼 것은 아니다. 하지만 유투브 영상 몇 개만으로 사람의 실력을 넘어서는건 상당히 의미가 있다고 본다. 유투브로부터 에이전트가 학습하는 법을 배울 수 있을 경우 앞으로 무엇이 가능할지 상상해보라.

<br/>

## 3. Introduction
---
- 사람은 비디오를 시청함으로서 많은 것들은 배운다. 온라인 demonstration으로부터 knowledge를 transfer 하는데 뛰어나다.
- 사람이 이렇게 학습할 수 있다는 것은 AI 연구의 대상이 되어왔다. 
- 이 논문에서는 아직 풀리지 않은 Atari 2600 게임에서 사람보다 플레이를 잘하는 에이전트를 소개한다. 이 에이전트는 noisy demonstration seqeunce로부터 self-supervised alignment를 학습한다.
- deep rl이 최근 많은 발전을 이루었지만 sparse reward 문제에서 exploration 하는 것은 여전히 challenge하다.
- 몬테주마 리벤지 게임은 point를 주는 아이템을 방에서 방을 오가며 수집해야하는 게임이다. 따라서 이런 게임에서는 epsilon greedy 방식의 exploration은 안먹힌다.
  - 예를 들어 몬테주마 리벤지 게임에서 첫번째 보상을 받으려면 환경에서 100 스텝을 진행해야한다. 행동의 개수가 18개라면 이 때 가능한 action sequence의 개수는 100^18이다. 
- sparse reward 문제를 풀기 위한 접근은 크게 두 개로 나뉜다.
  - 첫번째는 intrinsic motivation 이다. 환경에서 주는 보상이 아닌 별도의 보상을 통해 에이전트가 탐험을 하도록 유도하는 것이다. 특정 상태나 action trajectory가 어떠한 기준에 따라 novel 혹은 informative한지 측정하는 것이다. 이건 방문했던 state space 중에서 흥미로운 부분을 re-explore 하는 것이라서 처음 만나는 환경에서 exploration을 guide하기는 힘들다. 따라서 초반에는 random하게 exploration한다. 
  - 두번째는 imitation learning이다. human demonstrator의 플레이 데이터를 활용하는 것이다. 이 데이터를 활용해서 학습하는 것은 상당히 유용하다. 사람은 게임을 플레이할 때 skull은 위험하다는 것을 알고 key가 문을 연다는 것을 아는데 이런 사실이 이미 사람이 inductive bias를 가지로 있다는 것이다.
- DQfD가 현재까지 hard exploration 아타리 게임에서 가장 높은 점수를 기록했다. DQfD에는 두 가지 한계가 존재한다. 
  - 첫번째. agent와 demonstrator 사이에 domain gap이 없다고 가정한다. 하지만 다음 그림과 같이 게임 플레이 영상과 유투브 영상들은 서로 다 색깔 등에서 다르다. 즉 domain gap은 존재하는 경우가 많다. 
  <img src="https://www.dropbox.com/s/msiy6k1ggaceuoj/Screenshot%202018-11-02%2021.25.49.png?dl=1">
  - 두번째. agent는 demonstrator이 플레이하면서 취했던 정확한 행동과 받았던 보상들을 다 알아야한다. 이런 한계들은 강화학습을 위해 구성된 인공적인 환경에서는 문제가 되지 않는다. 하지만 우리는 이 한계를 뛰어넘는 방법을 소개할 것이다. 
- 여러개의 demonstration 사이에 존재하는 domain gap을 극복하는 방법을 소개할 것이다. 여기에 self-supervised classification task가 사용될 것이다. 
- 이 task를 통해 common representation을 학습할 수 있다. 
- 우리가 제시하는 방법은 frame 단위로 align이 안맞아도 된다.
- class label 같은 annotation도 필요없다.
- cycle consistency를 통해 학습한 embedding을 평가하는 방법도 제시한다.
- embedding을 통해 auxiliary imitation loss를 에이전트에게 준다. 이 loss를 통해 에이전트는 demonstrator에 대한 사전지식없이 hard exploration 문제를 해결할 수 있다.  


<br/>

## 4. Related Work
---
- domain gap 문제를 해결하기 위해 지금까지 여러가지 시도가 있었다. 
  - frame-by-frame alignment가 맞다면 CCA나 DCTW나 TCN 같은 방법을 통해 common representation을 학습할 수 있다. 
  - 유투브 영상같은 경우는 alignment가 안맞는다.
  - 이런 경우에는 여러 도메인에서의 shared auxiliary objective 를 푸는 방법이있다. 어떤 논문에서는 여러 도메인에서 같은 scene classification task를 같은 모델로 푼다면 alignment 문제를 해결할 수 있다는 것을 소개했다. 하지만 우리는 여기서와 달리 category guided supervision을 사용하지 않았다. 우리는 labeled data없이 이 문제를 풀었다. 즉 self-supervision인것이다.
  - single-view TCN이라는 논문도 있다. 이것 또한 paired training data가 필요없는 self-supervised task이다. 우리는 이 논문과 triplet-based ranking 대신 temporal classification을 사용한다는 점에서 다르다. 이 점 때문에 하이퍼 파라메터에 민감하지 않다.
  - L3-Net이라는 것도 있다. 여기서는 vision과 sound modality의 alignment를 학습한다. 하지만 우리는 multiple audio-visual sequence를 align하는 것을 배운다. multimodal alignment를 self-supervised objective로 삼는다. (뭐가 다른건지..)

- 우리는 TCN과 L3-net을 둘 다 활용했다. 
- 우리는 demonstrator의 행동을 설명해주는 보상함수를 학습한다고 할 수도 있다. 이렇게 보면 inverse-rl과도 비슷하다. 하지만 우리는 demonstrator의 action과 reward를 몰라도 학습하므로 좀 더 복잡한 설정이라고 볼 수 있다. 

<br/>

## 5. Closing the domain gap
---

이 논문이 하고 싶은 것은 다음 그림과 같다. 왼쪽이 실제 에이전트가 있는 게임화면이고 나머지가 유투브 영상이다. 이 유투브 영상들을 통해 에이전트가 게임 플레이하는 법을 학습할 것이다. 하지만 문제는 유투브 영상끼리 frame이 맞지 않고 align도 안 맞다는 점이다. 또한 밑에 그림에서 볼 수 있듯이 유투브 영상끼리도 색상이 다르다. 
<img src="https://www.dropbox.com/s/rr3wrmhuuijnfmc/Screenshot%202018-07-26%2023.08.06.png?dl=1">

따라서 이 논문에서는 이 영상들로부터 바로 에이전트를 학습시키는게 아니라 단계를 나눈다. 일단 영상들로부터 어떠한 공통적인 representation을 학습한다. 이 때 사용하는 방법을 TDC, CMC라고 부른다. TDC는 Temporal distance classification으로 영상에서 frame 끼리 얼마나 시간적 차이가 있는지 예측하는 것을 통해 representation을 학습하는 방법이다. CMC는 Cross-modal temporal distance classification으로 유투브 영상에 포함되어있는 소리 정보를 통해 TDC와 비슷하게 소리끼리의 시간 차이를 예측하는 것을 통해 representation을 학습하는 방법이다. representation을 학습하면 그 정보를 가지고 imitation을 위한 reward를 생성한다. 이제 어떻게 representation을 학습하는지 살펴보자.

### 5.1 TDC
다음 그림으로 살펴보는 것이 이해하기 가장 좋다. 
<img src="https://www.dropbox.com/s/117n8s3ruva0y40/Screenshot%202018-07-26%2023.23.09.png?dl=1">

위 그림에서 초록색 부분이 TDC이다. video가 있으면 그 video는 여러 frame으로 구성되어있을 것이다. 그 frame들 사이의 시간적 차이를 prediction 하는 unsupervised task로 학습하는 것이다. 이 과정에서 모델은 의미있는 representation을 학습하게 된다. 

시간은 discrete하게 몇 개의 구간으로 나누는데 논문에서는 6개의 구간으로 나누었다. 
<img src="https://www.dropbox.com/s/w5oezcrlelh5u64/Screenshot%202018-07-26%2023.25.51.png?dl=1">

즉, 두 개의 image가 주어지면 모델은 그 두 개를 입력받아서 6개의 category 중 하나로 classify하는 것이다. v와 w는 서로 다른 두 frame을 의미한다. v와 w가 각각 phi라는 일반적인 CNN 모델을 통해 feature(embedding)으로 변환이 되면 이 feature 들 간에 element-wise multiplication을 한다. 그리고 MLP를 통과해서 6개의 output 노드를 통해 각 category일 확률을 output으로 내보낸다. 결과를 가지고 다음과 같이 cross entropy loss를 만든다. y는 label이고 y^은 prediction이다. 

<img src="https://www.dropbox.com/s/cxtisa50sav4scu/Screenshot%202018-07-26%2023.30.54.png?dl=1">

visual embedding 모델과 classifier 모델의 세부사항은 다음과 같다. 이미지 인풋은 random cropping을 하며 4장의 이미지를 stack해서 하나의 input을 만든다. embedding vector는 l2-normalize한다. 

<img src="https://www.dropbox.com/s/ld7cgjvysk2lc53/Screenshot%202018-07-26%2023.32.37.png?dl=1">
<img src="https://www.dropbox.com/s/68wk22d2ur9odbj/Screenshot%202018-07-26%2023.33.21.png?dl=1">

### 5.2 CMC
유투브 영상에서 소리는 의미있는 정보를 포함하고 있다. 특히 아타리 게임과 같은 게임의 경우 소리는 어떤 event의 발생을 의미하는 경우가 많다. 따라서 video frame 뿐만 아니라 소리에 대해서도 temporal distance classification을 수행한다. 다시 아래 그림을 살펴보자. 소리에 대해서 classification 할 때는 소리끼리 하는 것은 아니고 frame 하나 소리 하나가 주어지고 두 개 사이의 관계를 알아내는 것이다. 이 경우 category가 2개라서 모델은 소리가 해당 frame과 같이 일어나는 것인지 아닌지만 classify하면 된다. 이 때, 소리를 embedding하기 위해 additional embedding function이 필요하다.
<img src="https://www.dropbox.com/s/117n8s3ruva0y40/Screenshot%202018-07-26%2023.23.09.png?dl=1">

CMC의 모델에 대한 상세한 정보는 다음과 같다. stft + conv1d 라고 볼 수 있다.

<img src="https://www.dropbox.com/s/6pf66nz9d0b3hhb/Screenshot%202018-07-26%2023.57.12.png?dl=1">

학습할 때는 TDC loss와 CMC loss를 weight를 곱해서 더해서 하나의 loss로 만들어서 학습한다. 
<img src="https://www.dropbox.com/s/x2z7pv3frvgjps0/Screenshot%202018-11-02%2022.13.34.png?dl=1">

### 5.3 Cycle-consistency
이렇게 학습한 embedding이 얼마나 좋은 것인지를 판단하는 것이 어렵다. 이 논문에서는 CycleGAN에서 사용되었던 'cycle relation' 개념을 활용한다. 만약 두 개의 sequence V = {v1, v2, v3, ...}와 W = {w1, w2, w3, ...}가 있다고 하면 이 두 개의 sequence에 대해 다 embedding을 구할 수 있다. 이 때, 각 embedding에 대해 Euclidean distance를 구할 수 있다. 
<img src="https://www.dropbox.com/s/tqt3pqerfm2rl3n/Screenshot%202018-07-26%2023.42.36.png?dl=1">

이 때 특정 vi에 대해서 W 중에서 embedding 상에서 가장 거리가 짧은 것을 찾았더니 wj였다면 그 반대도 성립하는 것을 cycle consistency라고 한다. cycle consistency를 만족할 확률을 one-to-one alignment capacity라고 하며 이것을 통해 embedding이 잘 학습되었는지를 파악할 수 있다. 여러가지 embedding 모델들의 cycle consistency를 비교한 것은 다음과 같다. 역시 논문에서 제시한 방법이 one-to-one alignment capacity가 가장 높다. 3-cycle-consistency 또한 비교를 했는데 TDC+CMC가 제일 높다.
<img src="https://www.dropbox.com/s/ksu4ekly1qxtt8c/Screenshot%202018-07-26%2023.45.42.png?dl=1">

또한 embedding space를 시각화해볼 수 있는데 논문에서는 t-SNE를 가지고 시각화를 하였다. 색깔이 다른 것이 서로 다른 유투브 영상이라는 뜻이다. pixel만 가지고 embedding을 할 경우 완전 실패하고 TDC만 사용할 경우에 여러가지 영상끼리 align이 잘 안 맞는다. TDC에다 CMC까지 사용할 경우 아래 그림의 맨 오른쪽 처럼 서로 다른 색깔이 비슷하게 가는 것을 볼 수 있다. 맨 위에 첨부한 유투브 영상에 이와 관련된 부분이 있으니 찾아보길 바란다. 
<img src="https://www.dropbox.com/s/g1l52e890mlbkw4/Screenshot%202018-07-26%2023.47.43.png?dl=1">


<br/>

## 6. One-shot imitation from Youtube footage
---
이렇게 representation을 학습했다면 이제 이 representation을 통해 agent를 학습시키는 단계이다. 에이전트를 학습시킬 때는 유투브 영상 하나만 사용한다. 유투브 영상을 frame 마다 embedding을 하면 trajectory가 생긴다. 이 trajectory에서 16 frame 마다를 checkpoint로 정한다. 그 checkpoint 마다 다음 수식을 통해 reward를 정의한다.
<img src="https://www.dropbox.com/s/cb140iezg3hw765/Screenshot%202018-07-26%2023.53.59.png?dl=1">

학습에 사용되는 agent는 IMPALA를 사용했다. 여기서 약간 허탈하지만 (actor가 100개나 된다...) 충분히 의미는 있다고 생각한다. 학습하고 나서 embedding model의 마지막 convolution layer를 한 번 visualize 해봤다. 각 뉴런이 상당히 의미있는 부분들을 포착하는 것을 볼 수 있고 CMC가 좀 더 key item에 집중하는 것을 볼 수 있다. 이것을 통해 embedding이 잘 학습된 것을 볼 수 있다.

<img src="https://www.dropbox.com/s/jufq5uj53snqebh/Screenshot%202018-07-26%2023.59.22.png?dl=1">

<br/>

## 7. Implementation Details
---
temporal classification과 cross-modal classification에 둘 다 쓰이는 shallow network는 2-layer relu-activated MLP이다. 
<img src="https://www.dropbox.com/s/z27c5fq7oflyqbw/Screenshot%202018-11-02%2022.26.24.png?dl=1">

학습 데이터는 세 개의 유투브 동영상으로부터 추출했다. 우선 동영상으로부터 demonstration sequence를 추출하고 interval과 distance(시간차)를 함께 구한다. 구성한 데이터셋으로부터 데이터를 랜덤하게 추출한다. 모델은 Adam으로 lr=10-4d로 학습했다. batch size는 32이고 200,000 번 업데이트했다. 

imitation loss를 만들기 위해 N=16마다 checkpoint를 만들었다. checkpoint는 하나의 youtube 영상의 embedding의 sequence 상에서 정의된다. 우리는 agent를 imitation reward와 환경의 reward를 합해서 학습시켰다. IMPALA를 사용해서 학습했는데 이 때, 논문과 다른 점이 하나 있다. 우리는 모델의 마지막 conv output에 현재 agent와 남은 두 개의 checkpoint 까지의 거리를 concat했다.

<br/>

## 8. Analysis and Experiments
---
학습용으로 유투브 영상 3개를 사용했고 테스트용으로 1개의 유투브 영상을 사용했다. 각 유투브 영상은 3에서 10분 정도의 길이다. 중요한 것은 유투브 영상을 모을때 ALE 사용한 영상을 모으지 않았다는 것이다. preprocessing도 최소한으로 했다. 

유투브 영상사이의 또는 ALE 환경과의 domain 차이를 극복하기 위해 두가지가 필요하다. 
1. one-to-one alignment
2. meaningful abstraction

첫번째로 one-to-one alignment는 cycle-consistency를 통해서 확인할 수 있다. cycle consistency를 계산하기 전에 모델의 앞 layer에서 centered되고 l2-normalize된다. 2-way cycle consistency를 계산할 때는 training video 하나와 test video 하나를 비교했다. 

meaningful abstraction을 확인하기 위해서 game의 high-level information을 확인했다. 다음 그림을 통해 확인할 수 있다. 재밌는 결과를 확인할 수 있다. 왼쪽은 마지막 convolution layer에서 각 뉴런의 activation을 가시화한 것 이다. 어떤 neuron은 agent를, 어떤 놈은 적을, 어떤 놈은 inventory를 중요하게 보고 있는 것을 알 수 있다. 오른쪽은 마지막 layer의 모든 channel은 더한 결과이다. CMC가 있으면 key를 더 중요하게 보는 것을 알 수 있다. 

<img src="https://www.dropbox.com/s/hmhr7ef4u0jvfpf/Screenshot%202018-11-02%2022.42.01.png?dl=1">

다음은 학습곡선이다. 
<img src="https://www.dropbox.com/s/9cl14u0x77cfsj6/Screenshot%202018-11-02%2022.46.43.png?dl=1">

<br/>

## 8. 개인생각
결국 몬테주마 리벤지 게임을 학습하기 위한 새로운 reward를 설계하는데 사람이 하지 않고 유투브 영상으로부터 배운 representation으로 하겠다는게 논문의 핵심이다. 하지만 이 논문에서와 다르게 영상에서 소리가 의미가 없는 경우도 많다. 또한 이 경우는 시간과 색상의 alignment가 영상과 실제 환경이 다른 것이지 화면 자체는 동일하다고 볼 수 있다. 하지만 많은 경우 에이전트가 유투브 영상으로부터 배울 때 이러한 조건들이 만족하지 않을 수 있다. 그럴 경우 어떻게 유용한 embedding을 학습하고 그로부터 reward를 잘 만들어낼지는 좀 더 살펴봐야할 것 같다.

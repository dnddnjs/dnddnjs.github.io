---
layout: post
title: "CIFAR-10 정복 시리즈 0: 시작하기"
subtitle: 
categories: cifar10
tags: 
comments: true
---

## CIFAR-10 정복하기 시리즈 소개
- CIFAR-10 정복하기 시리즈 목차(클릭해서 바로 이동하기)
  - [CIFAR-10 정복 시리즈 0: 시작하기](https://dnddnjs.github.io/cifar10/2018/10/08/start_cifar10/)
  - [CIFAR-10 정복 시리즈 1: ResNet](https://dnddnjs.github.io/cifar10/2018/10/09/resnet/)
  - [CIFAR-10 정복 시리즈 2: DenseNet](https://dnddnjs.github.io/cifar10/2018/10/11/densenet/)
  - [CIFAR-10 정복 시리즈 3: Wide ResNet](https://dnddnjs.github.io/cifar10/2018/10/12/wide_resnet/)
  - [CIFAR-10 정복 시리즈 4: Shake-shake](https://dnddnjs.github.io/cifar10/2018/10/13/shake_shake/)
  - [CIFAR-10 정복 시리즈 5: PyramidNet](https://dnddnjs.github.io/cifar10/2018/10/24/pyramidnet/)
  - [CIFAR-10 정복 시리즈 6: Shake-Drop](https://dnddnjs.github.io/cifar10/2018/10/19/shake_drop/)
  - [CIFAR-10 정복 시리즈 7: NAS](https://dnddnjs.github.io/cifar10/2018/11/04/nas/)
  - [CIFAR-10 정복 시리즈 8: NASNet](https://dnddnjs.github.io/cifar10/2018/11/03/nasnet/)
  - [CIFAR-10 정복 시리즈 9: Auto-Augment](https://dnddnjs.github.io/cifar10/2018/10/31/autoaugment/)

- 관련 코드 링크
  - [pytorch cifar10 github code](https://github.com/dnddnjs/pytorch-cifar10) 

--- 
### 딥러닝과 비전
딥러닝만큼 다양한 application에 적용되는 기술을 찾기 어렵다. 2012년 AlexNet[^1]의 등장 이후로 딥러닝은 무섭게 발전했다. Image Classification에서의 성과는 거기서만 머무르지 않았다. 6년이 지난 지금 딥러닝을 빼놓고 기술을 말하기가 어려워졌다. 개발자가 아닌 일반인들도 딥러닝이라는 단어를 알 정도로 기술의 영향력이 커졌다. 다음은 2018년 가장 성공한 AI 스타트업 100개를 모아놓은 곳이다. 이 100개의 기업이 받은 투자는 100억 달러가 넘는다. 이 중에서 11개의 기업은 소위 "유니콘" 기업이 되었다. 교육, 헬스케어, IOT, 여행, 로보틱스 등 AI 스타트업이 걸치지 않은 분야가 없을 정도이다.  

<img src="https://www.dropbox.com/s/xhfhavvkppujqan/Screenshot%202018-11-04%2012.03.08.png?dl=1" width='500px'>
<center>이미지 출처: https://www.cbinsights.com/research/artificial-intelligence-top-startups/</center>

<br/>

많은 사람들이 연구를 위해 혹은 사업을 위해 딥러닝을 공부한다. 딥러닝이 여러 분야에서 성과를 내고 있지만 딥러닝의 기본은 vision 분야에서 나오는 경우가 많다. 따라서 딥러닝을 시작하는 사람들은 CS231n[^2]과 같은 강의를 통해 vision에서의 딥러닝으로 공부하기 시작한다. CS231n은 Stanford 대학교에서 학부생을 대상으로하는 강의이다. CS231n 강의에서 소개한 딥러닝을 이용한 application은 다음과 같다. 

- Deep Learning을 이용한 Vision 분야 application
  - Image Classification
  - Object Localization
  - Object Detection
  - Semantic Segmentation
  - Instance Segmentation

Image classification은 vision 분야에서 가장 간단한 application이다. 이미지가 입력으로 들어오면 Neural Network는 이미지가 어떤 class에 해당하는지를 예측하는 것이다. 다음 그림은 이미지를 분류하는 과정을 보여준다. Image classification은 상당히 간단한 task이지만 AlexNet, GoogleNet, VGG, ResNet, DenseNet, MobileNet 등 모두 이 task를 풀기 위해 개발된 모델이다. 이 모델들은 image classification 이외의 다른 application 혹은 아예 다른 분야에서도 모델 구조로 사용되는 경우가 많다.  

<img src="https://www.dropbox.com/s/s9acumwosy14ah9/Screenshot%202018-11-04%2012.26.14.png?dl=1" width='500px'>
<center>이미지 출처: http://cs231n.stanford.edu/</center>

<br/>

상대적으로 연구를 하기에 간단한 task이기 때문에 image classification은 정말 다양한 연구가 되어있다. 단순히 모델 구조 뿐만 아니라 optimizer, weight initialization, regularization, data augmentation 등에 대해 풍부한 연구 결과가 쌓여있다. 따라서 image classification은 딥러닝을 처음 공부할 때 시작하기 좋은 분야이다. 


<br/>

### 더 좋은 모델 만들기
Image classification은 단순히 분류 문제이기 때문에 모델을 평가하기 쉽다. 테스트 데이터에서 대해서 얼마나 class를 잘 예측했는지를 통해 모델의 성능을 평가할 수 있다. 물론 계산량이나 모델 크기와 같은 수치로도 모델의 성능을 평가하기도 한다. 하지만 이 글에서는 accuracy만을 모델의 성능이라고 가정하고 이야기를 진행할 것이다. 딥러닝 학계에서는 모델의 성능을 올리기 위해 "딥"이라는 단어가 말해주듯 '어떻게 하면 모델을 더 깊게 만들 수 있을까'라는 주제에 노력을 기울였다. 하지만 2014년 VGG 논문에서 16 층의 딥러닝 모델을 만들고 2015년 GoogLeNet에서 22층의 딥러닝 모델을 만들면서 단순히 층을 쌓는 것의 한계를 체감한다. 하지만 GoogLeNet으로부터 VGG처럼 단순히 층을 쌓지 않고 창의적으로 구조를 변형하는 것이 효과가 있다는 것을 알게 된다. 2015년에 등장한 Batch Normalization과 함께 ResNet은 Paradigm Shift를 가져왔다.



- 단순히 크기를 키우거나 더 깊게 한다고 잘 되는 것이 아니다.
- classification 잘하는 모델 만들기
  - 모델 구조
  - regularization
  - augmentation

<br/>

### 왜 CIFAR-10인가 
- classification dataset. 각 application에 적용되는 모델이 많은 경우 classification model의 구조를 가져오는 경우가 많음. 
- classification dataset에서 풍부한 연구가 되어있음
- 가장 가벼운 데이터셋이면서도 challenge하기 때문에 빠르게 흐름을 파악하기 좋음
- 최근 1%대로 error rate를 줄였기 때문에 어느정도 정복된 데이터셋. 따라서 정복하기까지의 과정을 살펴볼 수 있음
- 데이터셋이 많지 않기 때문에 generalization 관련 이슈를 어떻게 해결하는지 볼 수 있음
- 간단히 GPU 1개로 돌려볼만하기 때문에 실습하기 좋음

<br/>

### 이 post를 통해 기대하는 것
- 

[^1]: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf
[^2]: http://cs231n.stanford.edu/

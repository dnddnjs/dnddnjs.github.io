---
layout: post
title: "CIFAR-10 정복 시리즈 2: DenseNet"
subtitle: "Densely Connected Convolutional Networks"
categories: paper
tags: dl
comments: true
---

현재 딥러닝은 vision 분야에서 탁월한 성능을 보이며 NLP, Robotics, Sound 분야로 그 영향력을 확장해나가고 있다. 각 분야에서 딥러닝은 다른 모습으로 활용이 되고 있지만 그 기본이 되는 것은 거의 vision 분야에서 나온다. 따라서 vision 분야에서 기본이 되는 논문을 읽어보고 직접 구현해보는 것은 앞으로의 실력 향상에 상당한 도움이 된다. 이 post에서는 vision 분야의 특정 task들의 baseline이 되는 논문을 살펴보고 코드를 리뷰하고자 한다. 

image recognition에서는 다양한 benchmark가 존재한다. 그 중에서 CIFAR-10는 비교적 양이 적은 dataset이다. 그러면서도 도전적인 측면이 있기 때문에 vision 관련 모델을 테스트해보기 좋다. 따라서 CIFAR-10에서 State-of-the art를 했던 모델들을 쭉 살펴볼 것이다. 이 series의 이름은 CIFAR-10 정복 시리즈이다. 이 series와 관련된 코드는 다음 github repository에서 볼 수 있다. 

- [pytorch cifar10 github code](https://github.com/dnddnjs/pytorch-cifar10) 


## 논문 제목: Densely Connected Convolutional Networks [2016 August]

<img src="https://www.dropbox.com/s/n7peav3s50ontg9/Screenshot%202018-10-11%2016.11.03.png?dl=1">
- 논문 저자: Gao Huang, Zhuang Liu, Laurens van der Maaten, JKilian Q. Weinberger
- 논문 링크: [https://arxiv.org/pdf/1608.06993.pdf](https://arxiv.org/pdf/1608.06993.pdf)

<br/>

### abstract
- 단순히 말하자면 densenet은 resnet을 보다 깊게 쌓는 효과를 skip connection을 더 많이 쓰는 방법으로 얻는 것이다. 조금 더 나은 성능에 더 적은 parameter, 더 적은 computation이 필요하다.
- 다음 그림을 이해하면 densenet은 절반은 이해한 것이다. 각 "block"은 모든 앞의 block의 출력을 합해서 입력으로 받는다(더하기 아니고 concat). 그리고 그 block의 출력은 역시 그 이후의 모든 block의 입력이 된다.
- 보면 간단하지만 실제로 학습시켜서 resnet보다 더 좋은 성능을 내기는 어려웠을 것이다. 어떻게 했을까. 한 번 살펴보자.

<img src="https://www.dropbox.com/s/qlw9b3vad5osrqa/Screenshot%202018-10-11%2016.16.10.png?dl=1">

<br/>

### Introduction
- Resnet, Highway Networks, Stochastic depth(Resnet), FractalNets 모두 동일한 이야기를 하고 있다. 성능을 높이기 위해 앞쪽 layer에서 뒤 쪽 layer로 가는 short path를 써라. (그런거 보면 resnet의 아이디어는 간단하고 강력하다. 심지어 재현도 잘된다. 이런 아이디어는 사랑해야한다)
- l 번째 layer는 l개의 입력이 있고 그 layer의 출력도 L-l 개의 layer를 통과한다. 이렇게 하면 L(L+1) / 2의 connection이 생긴다. connection이 dense하게 많기 때문에 densenet이라 하겠다. 
- dense한 connection이 왜 좋을까? 반복되는 feature map 학습안하고 더 적은 파라메터로 네트워크를 학습할 수 있기 때문이라고 논문은 말한다.
- resnet의 변형체들을 보면 resnet layer 중에 많은 layer가 성능에 별 영향을 안주고 심지어 dropout 될 수도 있다고 한다. 이런 성향은 RNN과도 비슷하다. 대신 하나 하나의 layer가 독립적으로 parameter를 가지기 때문에 전체 크기가 큰 것이다.
- densenet은 특정 layer로 들어온 여러 information을 똭 분리해서 미분할 수 있다. 
- densenet의 최종 classifier는 모든 feature map을 보고 판단하게 된다.
- densenet은 기존 접근과는 다르게 feature의 재사용에 초점을 뒀다.
- 그러니까 이 모든 설명은 위 그림을 설명하는 것이다.

<br/>

### DenseNets
- H()는 convolution + BN + activation + pooling을 포함한 함수. x는 해당 layer의 output
- ResNet은 다음과 같이 표현 가능함.
<img src="https://www.dropbox.com/s/kamnx4362ntgrsn/Screenshot%202018-10-11%2016.41.56.png?dl=1">

- identity function과 H output이 summation으로 합쳐졌기 때문에 gradient의 flow가 원활하지 못하다.
- DenseNet은 다음과 같다.
<img src="https://www.dropbox.com/s/nojzgv0hg5kg61u/Screenshot%202018-10-11%2016.44.59.png?dl=1">




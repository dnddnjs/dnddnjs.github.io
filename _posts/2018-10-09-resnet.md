---
layout: post
title: "CIFAR-10 정복 시리즈 2: ResNet"
subtitle: "Deep Residual Learning for Image Recognition"
categories: cifar10
tags: dl
comments: true
---


## CIFAR-10 정복하기 시리즈 소개
CIFAR-10 정복하기 시리즈에서는 딥러닝이 CIFAR-10 데이터셋에서 어떻게 성능을 높여왔는지 그 흐름을 알아본다. 또한 코드를 통해서 동작원리를 자세하게 깨닫고 실습해볼 것이다. 

- CIFAR-10 정복하기 시리즈 목차(클릭해서 바로 이동하기)
  - [CIFAR-10 정복 시리즈 0: 시작하기](https://dnddnjs.github.io/cifar10/2018/10/07/start_cifar10/)
  - [CIFAR-10 정복 시리즈 1: ResNet](https://dnddnjs.github.io/cifar10/2018/10/09/resnet/)
  - [CIFAR-10 정복 시리즈 2: ResDrop](https://dnddnjs.github.io/cifar10/2018/10/13/shake_shake/)
  - [CIFAR-10 정복 시리즈 3: Shake-shake](https://dnddnjs.github.io/cifar10/2018/10/13/shake_shake/)
  - [CIFAR-10 정복 시리즈 4: PyramidNet](https://dnddnjs.github.io/cifar10/2018/10/24/pyramidnet/)
  - [CIFAR-10 정복 시리즈 5: Shake-Drop](https://dnddnjs.github.io/cifar10/2018/10/19/shake_drop/)
  - [CIFAR-10 정복 시리즈 6: ENAS](https://dnddnjs.github.io/cifar10/2018/11/03/enas/)
  - [CIFAR-10 정복 시리즈 7: Auto-Augment](https://dnddnjs.github.io/cifar10/2018/11/05/autoaugment/)

- 관련 코드 링크
  - [pytorch cifar10 github code](https://github.com/dnddnjs/pytorch-cifar10) 

<br/>

## CIFAR-10 정복 시리즈 2: ResNet
1. Toward Deeper Network
2. From 10 to 100 layers
3. From 100 to 1000 layers
4. Code Review

- 참고 문헌
  - [ResNet 논문](https://arxiv.org/pdf/1512.03385.pdf)
  - [ResNet 저자 발표 자료](https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf)

<br/>

## 1. Toward Deeper Network
Deep Learning은 Deep Neural Network를 사용한 Machine Learning이라고 풀어서 말할 수 있다. MNIST와 같은 데이터셋에서는 3개의 층을 가지는 CNN으로도 높은 classification 성능을 얻을 수 있다. 하지만 CIFAR이나 ImageNet과 같이 좀 더 도전적인 데이터셋에서는 **얕은** 네트워크로는 한계가 있다. 따라서 연구자들은 과거부터 Layer를 더 깊게 쌓으려고 했다. 2012년 AlexNet은 8층이었으며 2014년의 VGG는 19층 GoogleNet은 22층이었다. 2014년까지는 가장 깊은 Neural Network가 몇십층 정도의 깊이를 가진 것이다. 하지만 2015년에 나온 ResNet은 152 층을 쌓았으며 2015년 ILSVRC 대회에서 우승하였다. 한마디로 진정한 **Deep Learning**의 시대를 연것이다. 

<figure>
  <img data-action="zoom" src="https://www.dropbox.com/s/qqswcef7uu5u9pv/Screenshot%202018-11-14%2016.19.31.png?dl=1" width='500px'> 
  <figcaption> 
  https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf</figcaption>
</figure>

<br>

ResNet 논문에서는 152보다 더 깊은 1000 층 이상의 ResNet도 실험했다. 하지만 논문의 실험 결과에 의하면 110층의 ResNet보다 1202층의 ResNet이 CIFAR-10에서 성능이 낮다. 이런 문제를 지적하며 ResNet 저자인 Kaiming He는 2016년에 ResNet의 후속 논문을 발표했다. "Identity Mappings in Deep Residual Networks" 에서는 ResNet 내부 구조의 변경을 통해 110층, 164층의 ResNet보다 1001층의 ResNet의 성능을 높게 만들 수 있었다. 다음 표를 보면 CIFAR-10 데이터셋에서 ResNet-1001이 (1001층의 깊이를 가지는 ResNet을 의미한다) 기존의 다른 네트워크보다 성능이 좋은 것을 볼 수 있다. 

<figure>
  <img src="https://www.dropbox.com/s/ad7l497i9vapl6e/Screenshot%202018-11-14%2017.25.54.png?dl=1">
  <figcaption>
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  </figcaption>
</figure>

<br>

위 결과는 ResNet이 Image Classification task에서 거둔 성과를 보여준다. 하지만 ResNet 또는 **더 깊은 Network는 단지 image classification에서만 사용되는 것이 아니다**. Detection, Segmentation, Pose Estimation, Depth Estimation 등에서 일명 **Backbone**으로 사용된다. Backbone은 등뼈라는 뜻이다. 등뼈는 뇌와 몸의 각 부위의 신경을 이어주는 역할을 한다. 뇌를 통해 입력이 들어온다고 생각하고 팔, 다리 등이 출력이라고 생각한다면 backbone은 입력이 처음 들어와서 출력에 관련된 모듈에 처리된 입력을 보내주는 역할이라고 생각할 수 있다. 여러가지 task가 몸의 각 부분이라고 생각하면 ResNet과 같은 classification model은 입력을 받아서 각 task에 맞는 모듈로 전달해주는 역할이다. 결국 객체를 검출하든 영역들을 나누든 Neural Network는 입력 이미지로부터 다양한 feature를 추출해야한다. 그 역할을 backbone 네트워크가 하는 것이다. 따라서 기본적으로 image classification 모델에 대한 이해가 필요하다.

<figure>
  <img src="https://www.dropbox.com/s/dfz5m2va31zdklv/Screenshot%202018-11-14%2017.37.06.png?dl=1" width='500px'>
  <figcaption>
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  </figcaption>
</figure>


<br>
**Deep Learning이 학습을 잘하게 되는 것은 단순히 층을 쌓는다고 되는 것이 아니다**. 10층을 쌓을 때까지 그리고 10층에서 100층, 100층에서 1000층은 각 단계마다의 문제가 존재한다. ResNet은 10층에서 100층 그리고 100층에서 1000층 사이에 해당한다. 10층 정도까지는 AlexNet, VGG 정도로 볼 수 있다. 이 네트워크에서는 10층을 쌓기 위해 어떤 노력을 했을까? 꽤나 최근까지 Deep Learning이 나올 수 없었던 이유는 Neural Network를 학습시키는 것이 어렵다는 사실이 큰 비중을 차지한다. Neural Network는 여러 weight와 bias라는 parameter를 가진다. Neural network를 학습시킨다는 것인 이 parameter를 stochastic gradient descent로 업데이트 한다는 것을 의미한다. 

Gradient를 통해 weight를 업데이트 할 때 **gradient가 explode하거나 vanishing** 하는 경우가 많았다. Gradient가 안정적이지 않은 이유는 neural network에서 사용하는 activation function과 연관성이 있다. 다음 그림은 Neural network에서 많이 사용하는 tanh로 10개의 층을 가지는 간단한 뉴럴넷을 만들고 테스트한 과정이다. 작은 random number로 network의 weight를 initialize하면 모든 activation이 0이 되는 현상이 발생한다. 자세한 내용은 CS231n 강의를 참고하길 바란다.

<figure>
  <img src="https://www.dropbox.com/s/wiqnya6j40iwq27/Screenshot%202018-11-15%2012.47.31.png?dl=1">
  <figcaption>
    http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf
  </figcaption>
</figure>

<br>

모든 activation이 0이 되거나 -1 아니면 1로 saturate되는 문제를 해결하기 위해 **Xavier initialization[^1]**과 **He initialization[^2]**이 나왔다. He initilization은 ReLU를 activation function으로 사용하는 경우에 더 깊은 neural network가 학습가능하게 만든 방법이다. 즉, 깊은 neural network를 학습시키려면 신경써서 initialization을 해야했다. 하지만 **Batch-Normalization[^3]**이 나오면서 이런 흐름을 바꾸었다. Batch normalization의 저자인 Sergey Ioffe와 Christian Szegedy는 Neural network가 학습하기 어려운 이유를 internal covariate shift라고 주장한다. Internal covariate shift는 neural network가 학습하면서 각 층의 입력 분포가 계속 변하는 현상이다. 따라서 근본적으로 neural network를 학습하기 어렵다고 판단했다. 

Batch normalization 이름처럼 이 문제를 mini-batch마다 각 층의 input을 normalization하는 방법으로 어느정도 해결했다. Batch normalization을 사용하면 initialization을 크게 신경쓰지 않아도 된다. 또한 optimizer의 learning rate를 이전보다 더 높일 수 있다. 결과적으로 더 빠른 학습을 가능하게 한 것이다. Batch normalization 논문에서 저자는 GoogleNet[^4]에 batch normalization을 적용해서 성능을 평가했다. 다음 그림을 보면 BN이라고 써져있는 네트워크(BN + GoogleNet)이 Inception(GoogleNet) 보다 훨씬 더 빠르게 학습하는 것을 볼 수 있다. 심지어 batch normalization은 regularization 역할도 하기 때문에 Dropout을 사용하지 않아도 학습이 잘 되는 특성이 있다. 이 때부터 많은 neural network에서 dropout을 사용하지 않기 시작했다. 

<figure>
  <img src="https://www.dropbox.com/s/0vlv5prg0g1b95l/Screenshot%202018-11-15%2013.15.07.png?dl=1">
  <figcaption>
    http://proceedings.mlr.press/v37/ioffe15.pdf
  </figcaption>
</figure>

<br>

## 2. From 10 to 100 layers
### 2-1. Degradation
Initialization과 normalization은 graident가 vanishing하거나 exploding하는 문제를 잡아줘서 더 깊은 network의 학습이 가능하게 했다. 그보다 더 깊은 network를 학습할 때는 어떤 문제가 발생할까? VGG나 GoogleNet과 같이 잘 짜여진 neural network에 층을 더 쌓을 경우 **Degradation** 문제가 발생한다. Degradation은 neural network의 깊이는 증가하는데 training error가 증가하는 경우를 말한다. Degradation은 overfitting으로 인해 생기는 현상이 아니다. 다음 그림에서 보면 56 층의 network가 20층의 network보다 training error와 test error가 둘 다 높은 것을 볼 수 있다. 

<figure>
  <img src="https://www.dropbox.com/s/06xom9u3umodycz/Screenshot%202018-10-09%2021.16.01.png?dl=1" width="500px">
  <figcaption>https://arxiv.org/pdf/1512.03385.pdf</figcaption>
</figure>

<br>

Degradation 문제는 **Convolutional Neural Networks at Constrained Time Cost[^5]** 논문과 **Highway Networks[^6]** 논문에서 소개되었다. "Convolutional Neural Networks at Constrained Time Cost"는 ResNet의 저자인 Kaiming He의 논문이다. 이 논문에서 학습이나 테스트 시간이라는 제약 조건 아래 여러 neural network 구조를 비교한다. Inference time을 유지하면서 네트워크의 깊이를 늘리려면 filter의 개수나 filter의 사이즈를 줄여야한다. 제약조건 아래에서 실험을 하면 성능에 어떤 요인이 영향을 주는지 확인하기 좋다. 이 논문에서 발견한 사실은 이러한 제약조건 아래에서 **네트워크의 성능에 영향을 주는 것은 filter의 개수나 filter의 사이즈보다 네트워크의 깊이라는 것이다**. 

이렇게 네트워크 깊이가 중요한 만큼 네트워크의 층을 더 늘리려는 노력이 필요하다. 하지만 일정이상 네트워크의 깊이를 늘리면 네트워크의 정확도가 변화가 없거나 오히려 떨어지는 **degradation** 현상이 발생한다. 논문에서는 degradation 문제를 더 살펴보기 위해 time constraint 없이 네트워크 깊이만 늘려보면서 error rate의 변화를 살펴봤다. 논문에서 실험한 여러가지 모델 중에 D라는 모델을 사용해 실험하였다. ImageNet 데이터에 실험한 다음 표를 보면 D+4 까지는 error rate가 준다. 하지만 D+6, D+8의 경우 오히려 error rate가 늘어난다. 

<figure>
  <img src="https://www.dropbox.com/s/g7kmy4ih8hg8bn8/Screenshot%202018-11-16%2016.42.00.png?dl=1" width="500px">
  <figcaption>https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/He_Convolutional_Neural_Networks_2015_CVPR_paper.pdf
  </figcaption>
</figure> 

<br>

### 2-2. Highway Network
ResNet이 처음 Degradation 문제를 해결하기 위해 새로운 네트워크 구조를 제안한 것이 아니다. ResNet이 나온 2015년에 Highway Network 논문이 나왔다. Highway network 또한 "training deeper networks is not as straightforward as simply adding layers." 이라고 언급하며 단순히 깊이를 늘리는 것 이외의 방법이 필요하다고 말한다. Highway network는 **LSTM(Long Short-Term Memory Models)**의 구조에서 영감을 받아서 만든 네트워크이다. 

LSTM은 기존 RNN의 vanishing gradient 문제를 해결하기 위해 나타났다. RNN과 LSTM의 중요한 차이는 hidden state 이외의 cell state가 존재하는 것이다. Cell state는 일종의 **information highway**로 작동한다. 기존 RNN에서는 이전 step의 정보가 다음 step으로 넘어갈 때 많은 반드시 non-linear 연산을 거쳐야한다. 하지만 LSTM에서는 cell state에 저장된 정보가 다음 step으로 넘어갈 때 곱하기와 더하기 연산만 거친다. 따라서 처음의 cell state 정보가 오랫동안 남아있을 수 있다. 아래 그림에서 위가 RNN이고 아래가 LSTM이다. LSTM에서는 위를 관통하는 하나의 선이 있는 것을 볼 수 있는데 이게 cell state이다. 마치 고속도로와 같기 때문에 information highway라고 하기도 한다. 혹시 RNN과 LSTM의 작동 방식을 잘 모른다면 [Illustrated Guide to LSTM's and GRU's: A step by step explanation](https://youtu.be/8HyCNIVRbSU) 영상을 추천한다.

<figure>
  <img src="https://www.dropbox.com/s/g4rhf45rag1tfqv/Screenshot%202018-11-16%2017.23.54.png?dl=1" width='500px'>
  <figcaption>
    https://imgur.com/jKodJ1u
  </figcaption>
</figure>
<br>

LSTM에서는 시간에 따라 정보가 유지되도록 했다면 Highway network에서는 앞단의 layer에서의 정보가 뒷 단의 layer로 유지되도록 한 것이다. Neural network가 L개의 layer로 이루어져있다고 하자. 각 layer에서는 $$H$$라는 non linear transformation을 input $$x$$에 적용한다. 이 때 출력을 $$y$$라고 하면 각 layer에서의 연산은 다음과 같이 쓸 수 있다. 

$$y = H(x, W_H)$$

<br>

Highway network에서는 $$T$$와 $$C$$라는 새로운 nonlinear transform을 사용한다. $$T$$는 transform gate를 의미하고 $$C$$는 carry gate를 의미한다. Transform gate는 $$H$$ 연산을 거친 정보를 얼마나 반영할지에 대한 gate이다. Carry gate는 입력으로 들어왔던 $$x$$의 정보를 얼마나 유지할까에 대한 gate이다. 입력 $$x$$가 마치 LSTM의 cell state와 같다라고 생각하면 이해가 쉬울 것이다. 이 두 개의 gate는 0에서 1사이의 값을 가진다. 
새로운 정보를 **transform**을 더 많이 하려면 input $$x$$의 정보를 더 줄여야하고 input $$x$$의 정보를 더 **carry**하려면 carry gate의 값이 커져야한다. 따라서 $$C = 1 - T$$로 정의할 수 있다. 이것을 반영해서 Highway Network의 한 layer 연산은 다음과 같이 쓸 수 있다. $$T$$는 0에서 1사이의 값을 가지기 때문에 sigmoid function을 사용한다. 즉, $$T(x) = \sigma(W_Tx + b_T)$$이다. 

$$y = H(x, W_H)\cdot T(x, W_T) + x\cdot (1-T(x, W_T))$$


<br>

- plain과 highway 비교한 결과를 소개
- inception도 일종의 shortcut connection?


### 2-3. ResNet






## 3. From 100 to 1000 layers

- 이 논문에서는 degradation 문제를 해결하기 위해 "deep residual learning" 구조를 제안함
- 원래 H(x) = F(x) 를 mapping 했다면 residual connection을 사용하면 H(x) = F(x) + x를 mapping 함.

<img src="https://www.dropbox.com/s/6acjshs76hb08gj/Screenshot%202018-10-09%2021.20.46.png?dl=1">

- 원래 mapping 보다 residual mapping이 더 최적화하기 쉽다고 가정
- 만약 identity mapping이 최적이면 그저 네트워크를 0이 되게 만들면 됌
- 밑에서 돌아서 위로 붙는 연결을 "shortcut connections"라고 함

<br/>

### Deep Residual Learning

- H(x) = F(x) 에서 H(x) = F(x) + x 로 mapping을 바꾸면 결과적으로 approximate하는 것은 같음. 하지만 학습과정은 상당히 다름
- 이전 실험결과(layer가 늘었을 때 training error가 늘어나는 현상)는 여러 nonlinear layer가 identity mapping을 하기 어려워한다는 것을 보여줌
- 만약 shortcut connection의 dimension이 같다면 다음과 같음
<img src="https://www.dropbox.com/s/wpflqqr7m86osid/Screenshot%202018-10-09%2021.39.11.png?dl=1">

- 만약 shortcut connection의 dimension이 다르다면 다음과 같음
<img src="https://www.dropbox.com/s/zefxlnbgex3pk2g/Screenshot%202018-10-09%2021.39.36.png?dl=1">

- residual function F로 2 layer를 써도 되고 그 이상을 써도 됌. 하지만 1 layer면 direct mapping이랑 비슷함.
- F + x는 fully-connected layer에서는 element-wise addition / convolutional layer에서는 channel-wise addition

- 비교 실험을 하기 위해 plaing baseline을 도입.
  - VGG 형태를 가져옴. 3x3 filter를 사용
  - output feature map size가 같다면 filter 개수도 같음
  - 만일 feature map size가 반으로 줄면 filter 개수는 두 배로 늘림
  - down-sampling은 stride 2 convolution으로 함
  - 마지막에 global-average pooling으로 1000개 class output 만듬

- VGG보다 fewer filters & lower complexity
- 다음은 VGG & Plain net & Resnet 비교 그림

<img src="https://www.dropbox.com/s/eqfw171k9kn7gik/Screenshot%202018-10-09%2021.49.14.png?dl=1">

- 위 그림에서 resnet의 경우 점선으로 표시되는 shortcut이 있음. 이 shortcut은 dimension을 높이는데 1x1 convolution을 사용함.
- Implementation detail
  - 이미지의 짧은 변을 256에서 480 사이의 특정 숫자로 rescale함
  - rescale한 이미지에 대해 224, 224로 random crop
  - horizontal flip 적용
  - per-pixel mean을 뺌
  - standard color augmentation 적용
  - batch normalization 적용
  - SGD with 256 mini batch leraning rate 0.1
  - lr는 error plateou마다 0.1배씩 감소
  - weight decay는 0.0001, SGD momentum은 0.9
  - test 할 때는 10-crop testing
  - multi-scale 사용 (224, 256, 384, 480, 640)
  - training images : 128만개
  - validation images : 5만개
  - test images : 10만개

<br/>

### Experiment
- plain net과 resnet은 18-layer, 34-layer를 테스트함
- Image net에 적용한 네트워크 architecture는 다음과 같음

<img src="https://www.dropbox.com/s/5fawzywoqa4v7wm/Screenshot%202018-10-10%2000.32.49.png?dl=1">

- plain net의 경우 iter & error 그래프에서 34-layer가 더 error가 높은 것을 볼 수 있다.
- resnet의 경우 34-layer가 더 error가 낮을 것을 볼 수 있다.
<img src="https://www.dropbox.com/s/g3uljkfs2eu78w1/Screenshot%202018-10-10%2000.34.43.png?dl=1">

- batch-norm을 통해 forward와 backward에서 signal가 사라지는 것을 방지함
- resnet은 어느정도 degradation 문제를 해결
- 18-layer 정도에서는 plain net과 resnet은 비슷한 성능을 보임. resnet-18이 좀 더 빠르게 수렴한다는데 별 차이없어 보임.
- 제대로 resnet을 활용하려면 resnet-34 이상을 쓰는 것이 좋아 보임. resnet-152 까지는 깊게 쌓을수록 더 성능이 좋아짐. 34, 50, 101, 152는 magic number가 아닌가 싶음.
- 다음은 resnet 실험 결과들. shortcut 변화에 따른 실험결과가 흥미롭다. shortcut에 변화를 줄 수 있는건 결국 dimension이 변할 때인데 이 때 (A) zero-padding을 하거나 (B) projection을 사용할 수 있다. A, B, C 변화에 따른 성능 차이가 미미하므로 shortcut의 방법은 크게 중요하지 않음을 알 수 있다. C의 경우 모델 크기만 늘리게 되므로 논문에서는 B를 사용한다.
<img src="https://www.dropbox.com/s/mddj8rhcnr12fjp/Screenshot%202018-10-10%2000.43.50.png?dl=1">
<img src="https://www.dropbox.com/s/eg4db2yz3mr7qhe/Screenshot%202018-10-10%2000.44.12.png?dl=1">

- resnet-34까지는 다음 그림의 왼쪽처럼 block을 만들고 resnet-50부터는 오른쪽처럼 block을 만든다. 더 깊게 네트워크를 쌓기 위해 1x1 convolution을 사용한다. 그렇게하면 parameter 숫자의 증가를 어느정도 막을 수 있다. 152 layer까지 쌓아도 vgg보다 모델 크기가 작다고 한다..
<img src="https://www.dropbox.com/s/5ytxdqsq73alruz/Screenshot%202018-10-10%2000.46.16.png?dl=1">

- CIFAR-10 에서의 실험 결과는 다음과 같다. 최고 성능은 110 layer resnet이다. 
<img src="https://www.dropbox.com/s/pihvoi3pn1xyk48/Screenshot%202018-10-12%2022.55.37.png?dl=1">

<br/>

### 참고문헌
[^1]: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
[^2]: https://arxiv.org/pdf/1502.01852.pdf
[^3]: http://proceedings.mlr.press/v37/ioffe15.pdf
[^4]: https://arxiv.org/pdf/1409.4842.pdf
[^5]: https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/He_Convolutional_Neural_Networks_2015_CVPR_paper.pdf
[^6]: https://arxiv.org/pdf/1505.00387.pdf

### Suplementaty Material
- resnet의 skip connection을 분석하는 논문이 이후에 나왔다. [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf)
- 관련된 한국말로 된 post는 다음과 같다. [https://kangbk0120.github.io/articles/2018-01/identity-mapping-in-deep-resnet](https://kangbk0120.github.io/articles/2018-01/identity-mapping-in-deep-resnet)
- 딥러닝 모델은 일반적인 함수를 모사할 수 있다. https://arxiv.org/pdf/1402.1869.pdf
- he initialization. https://arxiv.org/pdf/1502.01852.pdf
- batch normalization. http://proceedings.mlr.press/v37/ioffe15.pdf

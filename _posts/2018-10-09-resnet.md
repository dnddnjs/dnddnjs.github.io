---
layout: post
title: "CIFAR-10 정복 시리즈 2: ResNet"
subtitle: "Deep Residual Learning for Image Recognition"
categories: cifar10
tags: dl
comments: true
---


## CIFAR-10 정복하기 시리즈 소개
CIFAR-10 정복하기 시리즈에서는 딥러닝이 CIFAR-10 데이터셋에서 어떻게 성능을 높여왔는지 그 흐름을 알아본다. 또한 코드를 통해서 동작원리를 자세하게 깨닫고 실습해볼 것이다. 

- CIFAR-10 정복하기 시리즈 목차(클릭해서 바로 이동하기)
  - [CIFAR-10 정복 시리즈 0: 시작하기](https://dnddnjs.github.io/cifar10/2018/10/07/start_cifar10/)
  - [CIFAR-10 정복 시리즈 1: ResNet](https://dnddnjs.github.io/cifar10/2018/10/09/resnet/)
  - [CIFAR-10 정복 시리즈 2: ResDrop](https://dnddnjs.github.io/cifar10/2018/10/13/shake_shake/)
  - [CIFAR-10 정복 시리즈 3: Shake-shake](https://dnddnjs.github.io/cifar10/2018/10/13/shake_shake/)
  - [CIFAR-10 정복 시리즈 4: PyramidNet](https://dnddnjs.github.io/cifar10/2018/10/24/pyramidnet/)
  - [CIFAR-10 정복 시리즈 5: Shake-Drop](https://dnddnjs.github.io/cifar10/2018/10/19/shake_drop/)
  - [CIFAR-10 정복 시리즈 6: ENAS](https://dnddnjs.github.io/cifar10/2018/11/03/enas/)
  - [CIFAR-10 정복 시리즈 7: Auto-Augment](https://dnddnjs.github.io/cifar10/2018/11/05/autoaugment/)

- 관련 코드 링크
  - [pytorch cifar10 github code](https://github.com/dnddnjs/pytorch-cifar10) 

<br/>

## CIFAR-10 정복 시리즈 2: ResNet
1. [Toward Deeper Network](#toward-deeper-network)
2. [From 10 to 100 layers](#from-10-to-100-layers)
3. [From 100 to 1000 layers](#from-100-to-1000-layers)

- 참고 문헌
  - [ResNet 논문](https://arxiv.org/pdf/1512.03385.pdf)
  - [ResNet 저자 발표 자료](https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf)

<br/>

---

## Toward Deeper Network
Deep Learning은 Deep Neural Network를 사용한 Machine Learning이라고 풀어서 말할 수 있다. MNIST와 같은 데이터셋에서는 3개의 층을 가지는 CNN으로도 높은 classification 성능을 얻을 수 있다. 하지만 CIFAR이나 ImageNet과 같이 좀 더 도전적인 데이터셋에서는 **얕은** 네트워크로는 한계가 있다. 따라서 연구자들은 과거부터 Layer를 더 깊게 쌓으려고 했다. 2012년 AlexNet은 8층이었으며 2014년의 VGG는 19층 GoogleNet은 22층이었다. 2014년까지는 가장 깊은 Neural Network가 몇십층 정도의 깊이를 가진 것이다. 하지만 2015년에 나온 ResNet은 152 층을 쌓았으며 2015년 ILSVRC 대회에서 우승하였다. 한마디로 진정한 **Deep Learning**의 시대를 연것이다. 

<figure>
  <img data-action="zoom" src="https://www.dropbox.com/s/qqswcef7uu5u9pv/Screenshot%202018-11-14%2016.19.31.png?dl=1" width='500px'> 
  <figcaption> 
  https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf</figcaption>
</figure>

<br>

ResNet 논문에서는 152보다 더 깊은 1000 층 이상의 ResNet도 실험했다. 하지만 논문의 실험 결과에 의하면 110층의 ResNet보다 1202층의 ResNet이 CIFAR-10에서 성능이 낮다. 이런 문제를 지적하며 ResNet 저자인 Kaiming He는 2016년에 ResNet의 후속 논문을 발표했다. "Identity Mappings in Deep Residual Networks" 에서는 ResNet 내부 구조의 변경을 통해 110층, 164층의 ResNet보다 1001층의 ResNet의 성능을 높게 만들 수 있었다. 다음 표를 보면 CIFAR-10 데이터셋에서 ResNet-1001이 (1001층의 깊이를 가지는 ResNet을 의미한다) 기존의 다른 네트워크보다 성능이 좋은 것을 볼 수 있다. 

<figure>
  <img src="https://www.dropbox.com/s/ad7l497i9vapl6e/Screenshot%202018-11-14%2017.25.54.png?dl=1">
  <figcaption>
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  </figcaption>
</figure>

<br>

위 결과는 ResNet이 Image Classification task에서 거둔 성과를 보여준다. 하지만 ResNet 또는 **더 깊은 Network는 단지 image classification에서만 사용되는 것이 아니다**. Detection, Segmentation, Pose Estimation, Depth Estimation 등에서 일명 **Backbone**으로 사용된다. Backbone은 등뼈라는 뜻이다. 등뼈는 뇌와 몸의 각 부위의 신경을 이어주는 역할을 한다. 뇌를 통해 입력이 들어온다고 생각하고 팔, 다리 등이 출력이라고 생각한다면 backbone은 입력이 처음 들어와서 출력에 관련된 모듈에 처리된 입력을 보내주는 역할이라고 생각할 수 있다. 여러가지 task가 몸의 각 부분이라고 생각하면 ResNet과 같은 classification model은 입력을 받아서 각 task에 맞는 모듈로 전달해주는 역할이다. 결국 객체를 검출하든 영역들을 나누든 Neural Network는 입력 이미지로부터 다양한 feature를 추출해야한다. 그 역할을 backbone 네트워크가 하는 것이다. 따라서 기본적으로 image classification 모델에 대한 이해가 필요하다.

<figure>
  <img src="https://www.dropbox.com/s/dfz5m2va31zdklv/Screenshot%202018-11-14%2017.37.06.png?dl=1" width='500px'>
  <figcaption>
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  </figcaption>
</figure>


<br>
**Deep Learning이 학습을 잘하게 되는 것은 단순히 층을 쌓는다고 되는 것이 아니다**. 10층을 쌓을 때까지 그리고 10층에서 100층, 100층에서 1000층은 각 단계마다의 문제가 존재한다. ResNet은 10층에서 100층 그리고 100층에서 1000층 사이에 해당한다. 10층 정도까지는 AlexNet, VGG 정도로 볼 수 있다. 이 네트워크에서는 10층을 쌓기 위해 어떤 노력을 했을까? 꽤나 최근까지 Deep Learning이 나올 수 없었던 이유는 Neural Network를 학습시키는 것이 어렵다는 사실이 큰 비중을 차지한다. Neural Network는 여러 weight와 bias라는 parameter를 가진다. Neural network를 학습시킨다는 것인 이 parameter를 stochastic gradient descent로 업데이트 한다는 것을 의미한다. 

Gradient를 통해 weight를 업데이트 할 때 **gradient가 explode하거나 vanishing** 하는 경우가 많았다. Gradient가 안정적이지 않은 이유는 neural network에서 사용하는 activation function과 연관성이 있다. 다음 그림은 Neural network에서 많이 사용하는 tanh로 10개의 층을 가지는 간단한 뉴럴넷을 만들고 테스트한 과정이다. 작은 random number로 network의 weight를 initialize하면 모든 activation이 0이 되는 현상이 발생한다. 자세한 내용은 CS231n 강의를 참고하길 바란다.

<figure>
  <img src="https://www.dropbox.com/s/wiqnya6j40iwq27/Screenshot%202018-11-15%2012.47.31.png?dl=1">
  <figcaption>
    http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf
  </figcaption>
</figure>

<br>

모든 activation이 0이 되거나 -1 아니면 1로 saturate되는 문제를 해결하기 위해 **Xavier initialization[^1]**과 **He initialization[^2]**이 나왔다. He initilization은 ReLU를 activation function으로 사용하는 경우에 더 깊은 neural network가 학습가능하게 만든 방법이다. 즉, 깊은 neural network를 학습시키려면 신경써서 initialization을 해야했다. 하지만 **Batch-Normalization[^3]**이 나오면서 이런 흐름을 바꾸었다. Batch normalization의 저자인 Sergey Ioffe와 Christian Szegedy는 Neural network가 학습하기 어려운 이유를 internal covariate shift라고 주장한다. Internal covariate shift는 neural network가 학습하면서 각 층의 입력 분포가 계속 변하는 현상이다. 따라서 근본적으로 neural network를 학습하기 어렵다고 판단했다. 

Batch normalization 이름처럼 이 문제를 mini-batch마다 각 층의 input을 normalization하는 방법으로 어느정도 해결했다. Batch normalization을 사용하면 initialization을 크게 신경쓰지 않아도 된다. 또한 optimizer의 learning rate를 이전보다 더 높일 수 있다. 결과적으로 더 빠른 학습을 가능하게 한 것이다. Batch normalization 논문에서 저자는 GoogleNet[^4]에 batch normalization을 적용해서 성능을 평가했다. 다음 그림을 보면 BN이라고 써져있는 네트워크(BN + GoogleNet)이 Inception(GoogleNet) 보다 훨씬 더 빠르게 학습하는 것을 볼 수 있다. 심지어 batch normalization은 regularization 역할도 하기 때문에 Dropout을 사용하지 않아도 학습이 잘 되는 특성이 있다. 이 때부터 많은 neural network에서 dropout을 사용하지 않기 시작했다. 

<figure>
  <img src="https://www.dropbox.com/s/0vlv5prg0g1b95l/Screenshot%202018-11-15%2013.15.07.png?dl=1">
  <figcaption>
    http://proceedings.mlr.press/v37/ioffe15.pdf
  </figcaption>
</figure>

<br>

---

## From 10 to 100 Layers

### Degradation
Initialization과 normalization은 graident가 vanishing하거나 exploding하는 문제를 잡아줘서 더 깊은 network의 학습이 가능하게 했다. 그보다 더 깊은 network를 학습할 때는 어떤 문제가 발생할까? VGG나 GoogleNet과 같이 잘 짜여진 neural network에 층을 더 쌓을 경우 **Degradation** 문제가 발생한다. Degradation은 neural network의 깊이는 증가하는데 training error가 증가하는 경우를 말한다. Degradation은 overfitting으로 인해 생기는 현상이 아니다. 다음 그림에서 보면 56 층의 network가 20층의 network보다 training error와 test error가 둘 다 높은 것을 볼 수 있다. 

<figure>
  <img src="https://www.dropbox.com/s/06xom9u3umodycz/Screenshot%202018-10-09%2021.16.01.png?dl=1" width="500px">
  <figcaption>https://arxiv.org/pdf/1512.03385.pdf</figcaption>
</figure>

<br>

Degradation 문제는 **Convolutional Neural Networks at Constrained Time Cost[^5]** 논문과 **Highway Networks[^6]** 논문에서 소개되었다. "Convolutional Neural Networks at Constrained Time Cost"는 ResNet의 저자인 Kaiming He의 논문이다. 이 논문에서 학습이나 테스트 시간이라는 제약 조건 아래 여러 neural network 구조를 비교한다. Inference time을 유지하면서 네트워크의 깊이를 늘리려면 filter의 개수나 filter의 사이즈를 줄여야한다. 제약조건 아래에서 실험을 하면 성능에 어떤 요인이 영향을 주는지 확인하기 좋다. 이 논문에서 발견한 사실은 이러한 제약조건 아래에서 **네트워크의 성능에 영향을 주는 것은 filter의 개수나 filter의 사이즈보다 네트워크의 깊이라는 것이다**. 

이렇게 네트워크 깊이가 중요한 만큼 네트워크의 층을 더 늘리려는 노력이 필요하다. 하지만 일정이상 네트워크의 깊이를 늘리면 네트워크의 정확도가 변화가 없거나 오히려 떨어지는 **degradation** 현상이 발생한다. 논문에서는 degradation 문제를 더 살펴보기 위해 time constraint 없이 네트워크 깊이만 늘려보면서 error rate의 변화를 살펴봤다. 논문에서 실험한 여러가지 모델 중에 D라는 모델을 사용해 실험하였다. ImageNet 데이터에 실험한 다음 표를 보면 D+4 까지는 error rate가 준다. 하지만 D+6, D+8의 경우 오히려 error rate가 늘어난다. 

<figure>
  <img src="https://www.dropbox.com/s/g7kmy4ih8hg8bn8/Screenshot%202018-11-16%2016.42.00.png?dl=1" width="500px">
  <figcaption>https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/He_Convolutional_Neural_Networks_2015_CVPR_paper.pdf
  </figcaption>
</figure> 

<br>

### Highway Network
ResNet이 처음 Degradation 문제를 해결하기 위해 새로운 네트워크 구조를 제안한 것이 아니다. ResNet이 나온 2015년에 Highway Network 논문이 나왔다. Highway network 또한 "training deeper networks is not as straightforward as simply adding layers." 이라고 언급하며 단순히 깊이를 늘리는 것 이외의 방법이 필요하다고 말한다. Highway network는 **LSTM(Long Short-Term Memory Models)**의 구조에서 영감을 받아서 만든 네트워크이다. 

LSTM은 기존 RNN의 vanishing gradient 문제를 해결하기 위해 나타났다. RNN과 LSTM의 중요한 차이는 hidden state 이외의 cell state가 존재하는 것이다. Cell state는 일종의 **information highway**로 작동한다. 기존 RNN에서는 이전 step의 정보가 다음 step으로 넘어갈 때 많은 반드시 non-linear 연산을 거쳐야한다. 하지만 LSTM에서는 cell state에 저장된 정보가 다음 step으로 넘어갈 때 곱하기와 더하기 연산만 거친다. 따라서 처음의 cell state 정보가 오랫동안 남아있을 수 있다. 아래 그림에서 위가 RNN이고 아래가 LSTM이다. LSTM에서는 위를 관통하는 하나의 선이 있는 것을 볼 수 있는데 이게 cell state이다. 마치 고속도로와 같기 때문에 information highway라고 하기도 한다. 혹시 RNN과 LSTM의 작동 방식을 잘 모른다면 [Illustrated Guide to LSTM's and GRU's: A step by step explanation](https://youtu.be/8HyCNIVRbSU) 영상을 추천한다.

<figure>
  <img src="https://www.dropbox.com/s/g4rhf45rag1tfqv/Screenshot%202018-11-16%2017.23.54.png?dl=1" width='500px'>
  <figcaption>
    https://imgur.com/jKodJ1u
  </figcaption>
</figure>
<br>

LSTM에서는 시간에 따라 정보가 유지되도록 했다면 Highway network에서는 앞단의 layer에서의 정보가 뒷 단의 layer로 유지되도록 한 것이다. Neural network가 L개의 layer로 이루어져있다고 하자. 각 layer에서는 $$H$$라는 non linear transformation을 input $$x$$에 적용한다. 이 때 출력을 $$y$$라고 하면 각 layer에서의 연산은 다음과 같이 쓸 수 있다. 

$$y = H(x, W_H)$$

<br>

Highway network에서는 $$T$$와 $$C$$라는 새로운 nonlinear transform을 사용한다. $$T$$는 transform gate를 의미하고 $$C$$는 carry gate를 의미한다. Transform gate는 $$H$$ 연산을 거친 정보를 얼마나 반영할지에 대한 gate이다. Carry gate는 입력으로 들어왔던 $$x$$의 정보를 얼마나 유지할까에 대한 gate이다. 입력 $$x$$가 마치 LSTM의 cell state와 같다라고 생각하면 이해가 쉬울 것이다. 이 두 개의 gate는 0에서 1사이의 값을 가진다. 
새로운 정보를 **transform**을 더 많이 하려면 input $$x$$의 정보를 더 줄여야하고 input $$x$$의 정보를 더 **carry**하려면 carry gate의 값이 커져야한다. 따라서 $$C = 1 - T$$로 정의할 수 있다. 이것을 반영해서 Highway Network의 한 layer 연산은 다음과 같이 쓸 수 있다. $$T$$는 0에서 1사이의 값을 가지기 때문에 sigmoid function을 사용한다. 즉, $$T(x) = \sigma(W_Tx + b_T)$$이다. 만약 $$T$$가 0이라면 입력인 $$x$$가 그대로 출력으로 나간다. $$T$$가 1이라면 $$H$$의 출력이 해당 layer의 출력이 된다.

$$y = H(x, W_H)\cdot T(x, W_T) + x\cdot (1-T(x, W_T))$$

<br>

Highway network를 간단히 코드로 살펴보면 다음과 같다. self.gate와 self.nonlinear는 각각 $$W_T, W_H$$라고 볼 수 있다. 이 코드가 하나의 layer이고 이런 layer를 쭉 쌓으면 Highway network가 된다. 코드를 보면 직관적으로 Highway network를 이해할 수 있을 것이다. 

~~~python
gate = F.sigmoid(self.gate(x))
nonlinear = F.relu(self.nonlinear(x))

x = gate * nonlinear + (1 - gate) * x
~~~

<br>

Highway network의 경우 논문에서 fully connected layer에 대해서만 실험하였다. 이 논문에서는 plain network(information highway가 없는 일반 네트워크)와 highway 네트워크를 깊이를 다르게하며 성능을 비교했다. 아래 그림을 보면 네트워크의 깊이가 10에서 100으로 달라질 때 plain network와 highway network의 학습 과정이 어떻게 다른지 볼 수 있다. 네트워크의 깊이가 얕을 때는 plain network가 더 좋은 성능을 보인다. 하지만 네트워크가 깊어질수록 highway network의 성능이 plain network의 성능보다 더 높아진다. 사실 **highway network만 본다면 깊이가 늘어나는 것에 따라 네트워크의 error rate가 거의 변하지 않는 것**을 볼 수 있다. 이 실험결과를 통해 information highway와 같이 이전 layer의 정보를 다음 layer에 전해주는 방식이 layer를 더 깊게 쌓는데 효과가 있음을 알 수 있다.  

<figure>
  <img src="https://www.dropbox.com/s/gw5ldeetlvdlfdk/Screenshot%202018-11-17%2022.57.15.png?dl=1">
  <figcaption>
    https://arxiv.org/pdf/1505.00387.pdf
  </figcaption>
</figure>

<br>

### ResNet
ResNet 논문 또한 Highway network와 같이 degradation 문제를 해결하기 위한 방법을 제안한다. ResNet 논문에서도 degradation 현상을 실험을 통해 확인했다. 다음 그림에서 20-layer와 56-layer(뒤에서 이 네트워크의 구조를 설명할 것이다. 학습은 CIFAR-10에 대해서 한 것이다) 의 training error와 test error의 차이를 볼 수 있다. 네트워크의 층이 더 깊어졌을 때 오히려 training error와 test error 둘 다 높아지는 것을 볼 수 있다. 따라서 이 현상은 overfitting이 아닌 degradation 문제이다. ResNet 논문과 Highway network 논문이 둘 다 degradation 문제를 해결하려 한 것이라면 어떤 점이 다를까? ResNet과 Highway network의 중요한 차이점은 (1) **gate를 학습하는 것이 아니라 residual learning을 하는 것** (2) **CNN에 적용했다는 점** 이다. 

<figure>
  <img src="https://www.dropbox.com/s/je1s2bdftki4k7y/Screenshot%202018-11-17%2023.13.54.png?dl=1" width="500px">
  <figcaption>
    https://arxiv.org/pdf/1505.00387.pdf
  </figcaption>
</figure>

<br>
왜 20-layer 네트워크보다 56-layer 네트워크의 성능이 저하되는 것일까? 이론상 만일 20층부터 56층까지의 layer가 identity mapping을 해준다면 성능은 동일해야한다. 하지만 여러 층의 non-linear function을 identity mapping이 되도록 학습시키는 것은 쉽지 않다. Plain 네트워크는 정보가 흐를 수 있는 길이 하나밖에 없다. 다음 그림은 일반적인 plain 네트워크의 하나의 layer이다. Weight layer는 convolution layer이며 activation function은 relu이다. 이 plain 네트워크는 간단히 $$H(x)$$라고 쓸 수 있다. 만약 이 layer가 identity mapping을 학습한다면 $$H(x) = x$$가 되도록 $$H(x)$$를 맞추면 된다. 하지만 층이 깊은 네트워크에서 여러 개의 layer를 identity mapping이 되도록 학습한다는 것은 쉽지 않다. Identity function 자체는 상당히 간단한 함수이다. ResNet은 이 문제를 간단한 방법으로 해결한다.

<figure>
  <img src="https://www.dropbox.com/s/shwfv3c5ykymaeh/Screenshot%202018-11-18%2012.20.30.png?dl=1" width="400px">
  <figcaption>
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  </figcaption>
</figure>

<br>

Residual 이라는 것은 일종의 오차이다. 기존의 neural network가 direct mapping을 학습했다면 **Residual Learning**은 입력으로부터 얼만큼 달라져야하는지를 학습한다. $$H(x)$$가 학습해야하는 mapping이라면 $$H(x)$$를 $$H(x) = F(x) + x$$으로 새롭게 정의한다. 네트워크 학습 목표를 $$H(x)$$ 학습에서 $$F(x)$$ 학습으로 바꾸는 것이다. 먄약 identity mapping을 학습하고자 한다면 간단히 $$F(x)$$를 0으로 만들면 된다. 다음 그림은 Residual mapping을 학습하는 neural network을 보여준다. 기존 plain network의 차이는 identity x라고 써져있는 **shortcut connection** 부분이다. ResNet이 지금처럼 널리 사용되는 것은 성능 때문일수도 있지만 방법이 상당히 간단하다는데 있다. Shortcut connection이라는 것은 몇 개의 layer를 건너뛰는 것(skip)을 말한다. ResNet에서 Shortcut connection은 2개의 layer를 건너뛴다. Layer에 들어오는 입력이 shortcut connnection을 통해서 건너뛰면 layer를 지난 출력과 element-wise addition 한다. 

<figure>
  <img src="https://www.dropbox.com/s/odzu3mma87klx3n/Screenshot%202018-11-18%2012.20.53.png?dl=1" width="400px">
  <figcaption>
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  </figcaption>
</figure>

<br>

후속 논문인 "Identity Mappings in Deep Residual Networks"에서는 Residual block
ResNet의 한 layer를 수식으로 나타내면 다음과 같다. 수식에서 $$w$$ 밑에 $$i$$가 들어가는 것은 $$F$$가 하나의 layer가 아니라 여러 개의 layer라는 뜻이다. 위에서 언급했듯이 $$F$$는 2개의 layer로 이루어져있다. 수식으로는 $$F=W_2(\sigma (W_1 x))$$으로 사용한다. $$\sigma$$는 ReLU activation function을 의미한다. 

$$y = F(x, {W_i}) + x$$

<br>

보통은 Convolution layer 이후에 relu activation function 연산을 수행한다. ResNet 논문에서 중요한 점 중에 하나가 **Batch Normalization**을 사용한다는 것이다. Shortcut connection을 $$x$$라 했을 때 shortcut connection과 합쳐지는 것은 $$bn(conv(relu(bn(conv(x)))))$$가 된다. 이 연산과정을 다른 그림으로 보자면 다음과 같다. shortcut connection과 residual mapping을 더한 다음에 ReLU를 취하는 것을 볼 수 있다. 이렇게 하는 이유는 ReLU를 통과하면 +의 값만 남기 때문에 Residual의 의미가 제대로 유지되지 않기 때문이다. 이 연산을 하는 부분을 **Residual Block**이라고 부른다. 

<figure>
  <img src="https://www.dropbox.com/s/jbh4yhssogopgk0/Screenshot%202018-11-18%2013.52.58.png?dl=1" width='400px'>
  <figcaption>
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  </figcaption>
</figure>


Residual block을 코드로 보자면 다음과 같다. ResNet의 전체 코드는 뒤에서 살펴볼 것이다. conv1과 conv2는 2d convolution을 의미한다. 처음 입력 x는 self.bn2까지 거친 out과 element-wise addition을 한다. ResNet은 이 Residual Block이 여러 층으로 쌓은 neural network를 의미한다. 

~~~python
shorcut = x

out = self.conv1(x)
out = self.bn1(out)
out = self.relu(out)

out = self.conv2(out)
out = self.bn2(out)

out += shortcut
out = self.relu(out)
~~~

<br>

정확히 ResNet의 구조는 어떻게 되는 것일까? ResNet의 구조는 논문에서 ResNet과 비교하는 plain net과 거의 동일하다. 논문의 Plain net은 VGG의 구조에서 영감을 받아 만들었다. Plain net에서 여러 convolutional layer가 있는데 각 convolutional layer은 두 가지 규칙을 지킨다. (1) 만약 feature map size가 같다면 해당 convolutional layer의 filter 개수는 같다. (2) 만약 feature map size가 반으로 준다면 filter의 개수는 2배가 된다. 이 디자인 규칙은 ResNet도 그대로 가지고 있다. feature map size를 반으로 줄이는 것은 convolution을 할 때 stride를 2로 설정하는 것으로 한다. 마지막 convolutional layer의 output은 global average pooling을 통해서 pooling을 한다. 그 결과를 fully connected layer를 통해 class 개수만큼의 출력으로 변환한다. 


위에서 Residual block을 설명할 때 언급했던 것처럼 plain net에서도 batch normalization을 relu activation function 전에 사용한다. ResNet은 Plain network에서 두 개의 convolutional layer에 shortcut connection을 추가한 것이다. VGG와 plain network 그리고 ResNet의 구조를 비교한 그림은 다음과 같다. Plain network는 VGG에서 몇 개의 layer를 더 추가한 것임을 알 수 있다. Plain network는 VGG의 초반 몇 개의 3x3 convolution을 7x7 convolution으로 대체했다. 또한 global average pooling을 마지막 convolutional layer 출력에 적용하기 때문에 VGG에 비해 fully connected layer가 적다. ResNet은 plain network에서 두 개의 convolutional block을 skipping하는 shortcut connection을 추가한 것이다. 그림의 shortcut connection 중에서 점선인 것은 feature map size가 반으로 줄어든 경우를 의미한다. 다른 실선의 shortcut connection은 모두 parameter가 없지만 점선의 shortcut connection의 경우 1x1 convolution과 batch normalization을 적용하기 때문에 학습 대상인 connection이다. 

<figure>
  <img src="https://www.dropbox.com/s/eqfw171k9kn7gik/Screenshot%202018-10-09%2021.49.14.png?dl=1" width="400px">
  <figcaption>
  https://arxiv.org/pdf/1512.03385.pdf
  </figcaption>
</figure>

<br>


다음 표는 Plain network와 ResNet의 구조를 깊이마다 표로 정리한 것이다. 이 구조는 ImageNet 데이터에 대해 학습할 때 사용하는 구조이다. CIFAR 데이터의 경우 이미지 사이즈가 더 작기 때문에 몇가지 점이 다르다. ImageNet에서 사용된 네트워크 구조와 CIFAR 데이터에서 사용된 네트워크 구조가 어떻게 다른지는 뒤에서 이야기하겠다. 

<figure>
  <img src="https://www.dropbox.com/s/5fawzywoqa4v7wm/Screenshot%202018-10-10%2000.32.49.png?dl=1">
  <figcaption>
  https://arxiv.org/pdf/1512.03385.pdf
  </figcaption>
</figure>

<br>

ResNet을 제안한 이유는 degradation이라는 문제를 해결하기 위해서이다. 논문에서는 ImageNet 데이터에 대해서 여러가지 깊이로 plain net과 ResNet을 학습했다. 그 결과는 다음 그래프와 같다. 왼쪽은 plain network에 대한 실험 결과이고 오른쪽은 ResNet에 대한 실험 결과이다. Plain net의 경우 18층에서 34층으로 깊이를 늘리면 training error가 늘어나는 것을 볼 수 있다. 하지만 ResNet의 경우 18층에서 34층으로 깊이를 늘렸을 때 training error가 줄어든다. 이 실험결과를 통해 ResNet이 degradation 문제를 어느 정도 해결했다는 것을 알 수 있다. 

<figure>
  <img src="https://www.dropbox.com/s/g3uljkfs2eu78w1/Screenshot%202018-10-10%2000.34.43.png?dl=1">
  <figcaption>
    https://arxiv.org/pdf/1512.03385.pdf
  </figcaption>
</figure>

<br>

ResNet의 CIFAR-10에서의 성능을 살펴보자. 그 전에 하나 알아가야할 것이 있다. ResNet의 깊이가 점점 깊어지면 경우 parameter의 수가 너무 많아지기 때문에 residual block으로 다른 구조를 사용한다. Residual block의 원래 구조는 아래 그림의 왼쪽과 같다. 50층 이상인 ResNet에서는 오른쪽 그림과 같은 residual block을 사용한다. 1x1 convolution을 통해 filter size를 줄인 이후에 3x3 convolution을 하면 파라메터의 수를 아낄 수 있다. 이러한 구조의 residual block을 **bottelnet block**이라고 부른다. Bottleneck 구조를 사용해 ResNet의 내부 구조를 바꾸면 152 layer까지 쌓아도 vgg보다 모델 크기가 작다.
<figure>
  <img src="https://www.dropbox.com/s/blbivxvl6kjllfn/Screenshot%202018-11-18%2021.31.58.png?dl=1">
  <figcaption>
    https://arxiv.org/pdf/1512.03385.pdf
  </figcaption>
</figure>

<br>
CIFAR-10에서 성능은 다음과 같다. 110 개의 층을 가지는 ResNet이 test data에 대해서 6.43 %의 error rate를 기록했다. 이제 코드로 ResNet을 살펴볼 것이다. 

<figure>
  <img src="https://www.dropbox.com/s/pihvoi3pn1xyk48/Screenshot%202018-10-12%2022.55.37.png?dl=1" width="500px">
  <figcaption>
    https://arxiv.org/pdf/1512.03385.pdf
  </figcaption>
</figure>

<br>

### ResNet code review

- Implementation detail
  - 이미지의 짧은 변을 256에서 480 사이의 특정 숫자로 rescale함
  - rescale한 이미지에 대해 224, 224로 random crop
  - horizontal flip 적용
  - per-pixel mean을 뺌
  - standard color augmentation 적용
  - batch normalization 적용
  - SGD with 256 mini batch leraning rate 0.1
  - lr는 error plateou마다 0.1배씩 감소
  - weight decay는 0.0001, SGD momentum은 0.9
ResNet 

~~~python
import torch.nn as nn
import torch.utils.model_zoo as model_zoo

__all__ = ['ResNet', 'resnet54']



class BasicBlock(nn.Module):
  def __init__(self, inplanes, planes, stride=1, downsample=None):
    super(BasicBlock, self).__init__()
    self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, 
                         stride=stride, padding=1, bias=False) 
    self.bn1 = nn.BatchNorm2d(planes)
    self.relu = nn.ReLU(inplace=True)

    self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=3, 
                         stride=stride, padding=1, bias=False) 
    self.bn2 = nn.BatchNorm2d(planes)
    self.downsample = downsample
    self.stride = stride

  def forward(self, x):
    residual = x

    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)

    out = self.conv2(out)
    out = self.bn2(out)

    if self.downsample is not None:
      residual = self.downsample(x)

    out += residual
    out = self.relu(out)
    return out


class ResNet(nn.Module):
  def __init__(self, block, layers, num_classes=10):
    super(ResNet, self).__init__()
    self.inplanes = 16
    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, 
                         padding=1, bias=False)
    self.bn1 = nn.BatchNorm2d(16)
    self.relu = nn.ReLU(inplace=True)

    self.layer1 = self._make_layer(block, 16, layers[0])
    self.layer2 = self._make_layer(block, 32, layers[1], stride=2) 
    self.layer3 = self._make_layer(block, 64, layers[2], stride=2)  
    self.avg_pool = nn.AvgPool2d(8, stride=1)
    self.fc_out = nn.Linear(64, num_classes)

    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', 
                              nonlinearity='relu')
      elif isinstance(m, nn.BatchNorm2d):
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)

  def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if stride != 1 or self.inplanes != planes:
      downsample = nn.Sequential(
        nn.Conv2d(self.inplanes, planes, kernel_size=1, 
                stride=stride, bias=False),
        nn.BatchNorm2d(planes))
    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = planes
    for i in range(1, blocks):
      layers.append(block(self.inplanes, planes))
    return nn.Sequential(*layers)

  def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)

    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)

    x = self.avg_pool(x)
    x = x.view(x.size(0), -1)
    x = self.fc_out(x)
    return x


def resnet54(**kwargs):
  model = ResNet(BasicBlock, [9, 9, 9], **kwargs) 
  return model
~~~

data loader
~~~python
transforms_train = transforms.Compose([
  transforms.RandomCrop(32, padding=4),
  transforms.RandomHorizontalFlip(),
  transforms.ToTensor(),
  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])
transforms_test = transforms.Compose([
  transforms.ToTensor(),
  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

dataset_train = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transforms_train)
dataset_test = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transforms_test)
train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=args.batch_size, 
                        shuffle=True, num_workers=args.num_worker)
test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=100, 
                       shuffle=False, num_workers=args.num_worker)

~~~

optimizer scheduler
~~~python
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.1, 
                    momentum=0.9, weight_decay=1e-4)
step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[32000, 48000], gamma=0.1)
~~~

training loop
~~~python
net.train()

train_loss = 0
correct = 0
total = 0

for batch_idx, (inputs, targets) in enumerate(train_loader):
  step_lr_scheduler.step()
  inputs = inputs.to(device)
  targets = targets.to(device)
  outputs = net(inputs)
  loss = criterion(outputs, targets)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  train_loss += loss.item()
  _, predicted = outputs.max(1)
  total += targets.size(0)
  correct += predicted.eq(targets).sum().item()
~~~

<br>

## From 100 to 1000 Layers

asdfasdfasdfsd


### 참고문헌
[^1]: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
[^2]: https://arxiv.org/pdf/1502.01852.pdf
[^3]: http://proceedings.mlr.press/v37/ioffe15.pdf
[^4]: https://arxiv.org/pdf/1409.4842.pdf
[^5]: https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/He_Convolutional_Neural_Networks_2015_CVPR_paper.pdf
[^6]: https://arxiv.org/pdf/1505.00387.pdf

---
layout: post
title: "VAE tutorial"
subtitle: "cs231n lecture 13 & Auto-Encoding Variational Bayes paper"
categories: paper
tags: dl
comments: true
---

## 논문 제목: Auto-Encoding Variational Bayes [2013 March]

<img src="https://www.dropbox.com/s/1niug5qggbfatg7/Screenshot%202018-06-19%2021.36.15.png?dl=1">

- 논문 저자: Diederik P. Kingma (Universiteit van Amsterdam)
- 논문 링크: [https://arxiv.org/pdf/1312.6114.pdf](https://arxiv.org/pdf/1312.6114.pdf)
- 참고한 자료: [cs231n 2017 spring lecture 13](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf)

cs231n 강의 내용과 Kingma의 논문을 통해 Variational Auto-Encoder를 정리해봅니다. 다음과 같이 이 글은 진행합니다. 
 
1. CS231n 강의 내용
2. Kingma 논문 내용
3. VAE example 코드 리뷰


---
## 1. CS231n lecture 13: Generative Models
### 1.1 Supervised Learning vs Unsupervised Learning
13강 이전까지는 supervised learning 문제에 대한 딥러닝을 다뤘습니다. Supervised Learning은 데이터와 정답이 있는 경우 모델의 output이 정답과 같아지도록 loss function을 정의하고 학습합니다. Supervised Learning을 통해 Classification, Regression, Detection, Segmentation, image-captioning 등 다양한 task를 학습할 수 있습니다.  
<img src="https://www.dropbox.com/s/fe8fmjxdi7c510o/Screenshot%202018-06-19%2021.47.16.png?dl=1"> 

<br>
하지만 이 경우 항상 label이 있어야합니다. 그래서 데이터에 대한 cost가 큽니다. 딥러닝의 경우 모델의 성능이 데이터의 양과 질에 크게 의존하기 때문에 이에 대한 부담이 있습니다. 그렇다면 label이 없이 학습하는 모델은 없을까요? 그게 바로 Unsupervised Learning 입니다. Unsupervised Learning은 input이 되는 x 데이터만으로 학습할 수 있습니다. 주 목적은 x 데이터의 underlying hidden structure을 학습하는 것입니다. 개인적으로는 data의 meaningful representation을 학습하는 것이 중요하다고 생각해서 Unsupervised Learning을 관심있게 보고 있습니다. 

<img src="https://www.dropbox.com/s/xjv2wwwquno8n8j/Screenshot%202018-06-19%2021.50.49.png?dl=1">

<br>
Unsupervised Learning으로는 Clustering, dimensionality reduction, feature learning, density estimation 등을 할 수 있습니다. 지도학습과는 달리 아직 미개척 분야이며 풀어야할 문제가 많고 어쩌면 그 안에 보물을 담고 있을지도 모릅니다. Unsupervised Learning 중에 가장 유명한 것이 K-means와 Auto-Encoder 입니다. 하지만 Auto-Encoder를 그냥 사용하는 경우는 없고 보통 VAE(Variational Auto-Encoder)를 많이 사용합니다. VAE의 경우 Auto-Encoder와는 달리 데이터의 의미있는 representation을 학습할 수 있습니다. 하지만 Auto-Encoder를 처음 접하고 나서 VAE로 넘어갈 때 상당히 큰 벽이 있는 것 같습니다. 

VAE를 제대로 이해하려면 코드부터 보는 것이 아니라 세 가지를 알아야합니다. 

1. VAE는 Generative model이라는 것
2. latent variable이라는 것이 있으며 이것은 variational inference 통해 나온다는 것.
3. 이러한 것을 구현하기 위해 Auto-Encoder 구조를 가져온 것

<br>
### 1.2 Generative Model
VAE는 일종의 Generative model이라고 봐야합니다. Generative model이란 training data가 주어졌을 때 이 data가 sampling된 분포와 같은 분포에서 새로운 sample을 생성하는 model입니다. 즉 $$p_{model}(x)$$가 최대한 $$p_{data}(x)$$에 가깝게 만드는 것이 목표입니다. 이것을 어떻게 할 수 있을까요? 결국 얼마나 기존 모델과 가까운 것인가에 대한 지표를 만들어야하고 그 차이를 최소화하도록 gradient를 계산해서 업데이트할 것입니다. 

<img src="https://www.dropbox.com/s/ltfz827o0ltj611/Screenshot%202018-06-19%2021.58.21.png?dl=1">

<br>
다음 그림은 Generative Model에 대해 Ian Goodfellow가 정리한 도표입니다. Generative Model은 크게 Explicit Density와 Implicit Density 두 가지로 나눌 수 있습니다. Explicit Density 모델을 data를 샘플링한 모델의 구조를 명확히 정의를 합니다. 그 모델로부터 data를 sampling 하는 것입니다. 하지만 Implicit Density에서는 모델에 대한 explicit하게 정의하지 않습니다. 예를 들어, GAN의 경우 noise로부터 바로 data로의 transformation을 학습합니다. VAE는 data의 model의 density model을 explicit하게 정의해서 직접적으로 학습하는 경우라고 볼 수 있습니다. 

<img src="https://www.dropbox.com/s/ce7x00eq6eltvho/Screenshot%202018-06-19%2022.05.22.png?dl=1">

<br>

<center><img src="https://www.dropbox.com/s/t78ehuhvoor7rj1/Screenshot%202018-06-19%2022.49.03.png?dl=1" width=400px></center>
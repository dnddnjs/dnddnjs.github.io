---
layout: post
title: "VAE tutorial"
subtitle: "cs231n lecture 13 & Auto-Encoding Variational Bayes paper"
categories: paper
tags: dl
comments: true
---

## 논문 제목: Auto-Encoding Variational Bayes [2013 March]

<img src="https://www.dropbox.com/s/1niug5qggbfatg7/Screenshot%202018-06-19%2021.36.15.png?dl=1">

- 논문 저자: Diederik P. Kingma (Universiteit van Amsterdam)
- 논문 링크: [https://arxiv.org/pdf/1312.6114.pdf](https://arxiv.org/pdf/1312.6114.pdf)
- 참고한 자료: 
	- [cs231n 2017 spring lecture 13](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf)
	- [https://arxiv.org/pdf/1606.05908.pdf](https://arxiv.org/pdf/1606.05908.pdf)

cs231n 강의 내용과 Kingma의 논문을 통해 Variational Auto-Encoder를 정리해봅니다. 다음과 같이 이 글은 진행합니다. 
 
1. CS231n 강의 내용
2. Kingma 논문 내용
3. VAE example 코드 리뷰


---
## 1. CS231n lecture 13: Generative Models
### 1.1 Supervised Learning vs Unsupervised Learning
13강 이전까지는 supervised learning 문제에 대한 딥러닝을 다뤘습니다. Supervised Learning은 데이터와 정답이 있는 경우 모델의 output이 정답과 같아지도록 loss function을 정의하고 학습합니다. Supervised Learning을 통해 Classification, Regression, Detection, Segmentation, image-captioning 등 다양한 task를 학습할 수 있습니다.  
<img src="https://www.dropbox.com/s/fe8fmjxdi7c510o/Screenshot%202018-06-19%2021.47.16.png?dl=1"> 

<br>
하지만 이 경우 항상 label이 있어야합니다. 그래서 데이터에 대한 cost가 큽니다. 딥러닝의 경우 모델의 성능이 데이터의 양과 질에 크게 의존하기 때문에 이에 대한 부담이 있습니다. 그렇다면 label이 없이 학습하는 모델은 없을까요? 그게 바로 Unsupervised Learning 입니다. Unsupervised Learning은 input이 되는 x 데이터만으로 학습할 수 있습니다. 주 목적은 x 데이터의 underlying hidden structure을 학습하는 것입니다. 개인적으로는 data의 meaningful representation을 학습하는 것이 중요하다고 생각해서 Unsupervised Learning을 관심있게 보고 있습니다. 

<img src="https://www.dropbox.com/s/xjv2wwwquno8n8j/Screenshot%202018-06-19%2021.50.49.png?dl=1">

<br>
Unsupervised Learning으로는 Clustering, dimensionality reduction, feature learning, density estimation 등을 할 수 있습니다. 지도학습과는 달리 아직 미개척 분야이며 풀어야할 문제가 많고 어쩌면 그 안에 보물을 담고 있을지도 모릅니다. Unsupervised Learning 중에 가장 유명한 것이 K-means와 Auto-Encoder 입니다. 하지만 Auto-Encoder를 그냥 사용하는 경우는 없고 보통 VAE(Variational Auto-Encoder)를 많이 사용합니다. VAE의 경우 Auto-Encoder와는 달리 데이터의 의미있는 representation을 학습할 수 있습니다. 하지만 Auto-Encoder를 처음 접하고 나서 VAE로 넘어갈 때 상당히 큰 벽이 있는 것 같습니다. 

VAE를 제대로 이해하려면 코드부터 보는 것이 아니라 세 가지를 알아야합니다. 

1. VAE는 Generative model이라는 것
2. latent variable이라는 것이 있으며 이것은 variational inference 통해 나온다는 것.
3. 이러한 것을 구현하기 위해 Auto-Encoder 구조를 가져온 것

<br>
### 1.2 Generative Model
VAE는 일종의 Generative model이라고 봐야합니다. Generative model이란 training data가 주어졌을 때 이 data가 sampling된 분포와 같은 분포에서 새로운 sample을 생성하는 model입니다. 즉 $$p_{model}(X)$$가 최대한 $$p_{data}(X)$$에 가깝게 만드는 것이 목표입니다. 이것을 어떻게 할 수 있을까요? 결국 얼마나 기존 모델과 가까운 것인가에 대한 지표를 만들어야하고 그 차이를 최소화하도록 gradient를 계산해서 업데이트할 것입니다. 

<img src="https://www.dropbox.com/s/ltfz827o0ltj611/Screenshot%202018-06-19%2021.58.21.png?dl=1">

<br>
다음 그림은 Generative Model에 대해 Ian Goodfellow가 정리한 도표입니다. Generative Model은 크게 Explicit Density와 Implicit Density 두 가지로 나눌 수 있습니다. Explicit Density 모델을 data를 샘플링한 모델의 구조를 명확히 정의를 합니다. 그 모델로부터 data를 sampling 하는 것입니다. 하지만 Implicit Density에서는 모델에 대한 explicit하게 정의하지 않습니다. 예를 들어, GAN의 경우 noise로부터 바로 data로의 transformation을 학습합니다. VAE는 data의 model의 density model을 explicit하게 정의해서 직접적으로 학습하는 경우라고 볼 수 있습니다. 

<img src="https://www.dropbox.com/s/ce7x00eq6eltvho/Screenshot%202018-06-19%2022.05.22.png?dl=1">

<br>

다시 말하자면 다음과 같습니다. "density estimation"은 x라는 데이터만 관찰할 수 있을 때, 관찰할 수 없는 x가 샘플된 확률밀도함수(probability density function)을 estimate하는 것입니다. 예를 들어 다음 그림처럼 데이터 포인트 들이 찍혀있을 경우에 대해서 생각해보는 것입니다. 그러면 모델을 가지고 해야하는 일은 이 데이터 포인트들이 샘플링 됐을 것 같은 확률밀도함수를 찾아내는 것입니다. 
<center><img src="https://www.dropbox.com/s/t78ehuhvoor7rj1/Screenshot%202018-06-19%2022.49.03.png?dl=1" width='400px'></center>

<br>
이 확률밀도함수 즉 최대한 $$p_{data}(\mathcal{X})$$에 가까운 $$p_{model}(X)$$를 찾아냈을 때, $$p_{model}(X)$$을 통해 새로운 데이터를 "generate" 할 수 있습니다. 따라서 이런 것을 Generative Model이라고 부르는 것입니다. Generative Model 중에 가장 sharp 하고 realistic image를 생성하는 것은 GAN(Generative Adversarial Network)입니다. GAN은 여러 변형체들이 많은데 그 중에 하나의 학습된 모델을 가지고 생성한 데이터 예시입니다. 하지만 GAN의 경우 random noise로부터 데이터를 생성해내므로 데이터의 의미있는 representation을 학습할 수 없습니다. 이제 VAE에 대해서 살펴보겠습니다.

<img src="https://www.dropbox.com/s/rqxbf2jw3odz7hn/Screenshot%202018-06-19%2023.06.16.png?dl=1">

<br>
### 1.3 Variational Auto-Encoder
일단 probability density function을 정의합니다. $$p_{\theta}(x)$$라고 정의합니다. dataset은 $$X = [x^{(i)}]_{i=1}^N$$이라고 할 수 있습니다. 이 때, dataset의 datapoint($$x^{(i)}$$)는 i.i.d. 하다고 가정합니다. 이 density function은 $$\theta$$를 parameter로 가지고 있습니다. 이 정의의 의미는 $$\theta$$라는 parameter가 정해졌을 때 $$x$$라는 데이터가 나올 확률입니다. 이 확률을 최대화하는 것이 Generative Model 혹은 density estimation의 목표입니다. 이 식은 $$z$$라는 latent variable을 사용해서 다음과 같이 쓸 수 있습니다. 앞으로 우리는 이 식을 미분해서 그 미분값에 따라 stochastic gradient ascent를 할 것입니다.

$$p_{\theta}(x)=\int p_{\theta}(z)p_{\theta}(x|z)dz - (1)$$


이 식은 다음 그림으로 보면 더 이해가 잘 갑니다. VAE에서는 Auto-Encoder와는 달리 latent variable을 정의합니다. 왜 갑자기 "latent variable"이라는게 등장했을까요? 우리가 생성하고 싶은 데이터들은 상당히 차원이 높고 예를 들어 datapoint의 pixel 사이에 복잡한 관계가 있습니다. 따라서 pixel 사이의 관계를 확률모델로 모델링하는 것이 아니라 데이터를 표현하는 $$z$$ (mnist의 경우 어떤 숫자인지)가 있으면 그 $$z$$로부터 데이터를 생성하는 graphical model을 생각해보는 것입니다. 
[그림출처:http://pyro.ai/examples/vae.html](http://pyro.ai/examples/vae.html)
<center><img src="https://www.dropbox.com/s/jyiu96dd3tp8rue/Screenshot%202018-06-20%2000.53.19.png?dl=1" width="150px"></center>

(1) 식은 intractable 한데 그 이유는 가능한 모든 z에 대해 integral을 취하기 때문입니다. 

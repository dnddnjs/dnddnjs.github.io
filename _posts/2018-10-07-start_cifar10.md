---
layout: post
title: "CIFAR-10 정복 시리즈 0: 시작하기"
subtitle: 
categories: cifar10
tags: 
comments: true
---

## CIFAR-10 정복하기 시리즈 소개
CIFAR-10 정복하기 시리즈에서는 딥러닝이 CIFAR-10 데이터셋에서 어떻게 성능을 높여왔는지 그 흐름을 알아본다. 또한 코드를 통해서 동작원리를 자세하게 깨닫고 실습해볼 것이다. 

- CIFAR-10 정복하기 시리즈 목차(클릭해서 바로 이동하기)
  - [CIFAR-10 정복 시리즈 0: 시작하기](https://dnddnjs.github.io/cifar10/2018/10/07/start_cifar10/)
  - [CIFAR-10 정복 시리즈 1: ResNet](https://dnddnjs.github.io/cifar10/2018/10/09/resnet/)
  - [CIFAR-10 정복 시리즈 2: PyramidNet](https://dnddnjs.github.io/cifar10/2018/10/24/pyramidnet/)
  - [CIFAR-10 정복 시리즈 3: Shake-shake](https://dnddnjs.github.io/cifar10/2018/10/13/shake_shake/)
  - [CIFAR-10 정복 시리즈 4: Shake-Drop](https://dnddnjs.github.io/cifar10/2018/10/19/shake_drop/)
  - [CIFAR-10 정복 시리즈 5: ENAS](https://dnddnjs.github.io/cifar10/2018/11/03/enas/)


- 관련 코드 링크
  - [pytorch cifar10 github code](https://github.com/dnddnjs/pytorch-cifar10) 

--- 
### 딥러닝과 비전
2012년 AlexNet[^1]의 등장 이후로 딥러닝은 무섭게 발전했다. 딥러닝의 Image Classification에서 뛰어난 능력을 보여줬지만 그 자리에만 머무르지 않았다. 6년이 지난 지금 딥러닝을 빼놓고 기술을 말하기가 어려워졌다. 개발자가 아닌 일반인들도 딥러닝이라는 단어를 알 정도로 기술의 영향력이 커졌다. 다음은 2018년 가장 성공한 AI 스타트업 100개를 모아놓은 곳이다. 이 100개의 기업이 받은 투자는 100억 달러가 넘는다. 이 중에서 11개의 기업은 소위 "유니콘" 기업이 되었다. 교육, 헬스케어, IOT, 여행, 로보틱스 등 AI 스타트업이 걸치지 않은 분야가 없을 정도이다. AI라고 불리우는 기술은 대부분 딥러닝을 사용한다. AI라는 단어를 사용하지 않더라도 이미 제품이나 서비스를 가지고 있는 기업에서도 딥러닝을 광범위하게 사용한다. 이 점을 고려하면 딥러닝만큼 다양한 application에 적용되는 기술을 찾기 어렵다. 

<img src="https://www.dropbox.com/s/xhfhavvkppujqan/Screenshot%202018-11-04%2012.03.08.png?dl=1" width='500px'>

<center>이미지 출처: https://www.cbinsights.com/research/artificial-intelligence-top-startups/</center>

<br/>

이렇다보니 많은 사람들이 연구를 위해 혹은 사업을 위해 딥러닝을 공부한다. 딥러닝이 여러 분야에서 성과를 내고 있지만 딥러닝의 기본은 vision 분야에서 나오는 경우가 많다. 강화학습 또한 딥러닝을 통해 이전에는 하기 힘들었던 task를 에이전트가 학습한다. 그 중에 대표적인 적인 것이 OpenAI의 "Learning Dexterity[^2]"라는 연구이다. 이 연구가 하고자 하는 것은 시뮬레이션 상에서 로봇손이 어떤 물체를 손안에서 돌리는 것이다. 시뮬레이션 상에서 학습이 끝나면 실제 로봇손에서 같은 일을 수행한다. 로봇손이 어떻게 물체가 어떤 상태에 있는지 알 수 있을까? 바로 카메라로 그 물체를 찍고 그 물체의 이미지를 CNN으로 분석하기 때문이다. 이때 카메라 영상에서 물체의 위치와 회전을 예측하는 것을 ResNet 구조를 사용한 뉴럴넷 모델이 수행한다. 다음은 OpenAI 연구에 대한 동영상이다. 

[![Video Label](http://img.youtube.com/vi/jwSbzNHGflM/0.jpg)](https://youtu.be/jwSbzNHGflM?t=0s)

강화학습 뿐만 아니라 많은 곳에서 비전 기반의 딥러닝이 기본이 된다. 따라서 많은 사람들이 딥러닝을 시작할 때 비전 분야의 딥러닝을 공부한다. 그것이 CS231n 강의[^3]가 그렇게 인기가 많은 이유이다. CS231n은 Stanford 대학교에서 학부생을 대상으로하는 강의이다. Fei Fei Li나 Andrey Karphathy와 같은 훌륭한 연구자가 강의를 하며 온라인에 무료로 공개하였다. 이와 같은 강좌들이 많은 사람이 딥러닝을 더 쉽게 시작할 수 있도록 도와준다. CS231n 강의에서 소개한 딥러닝을 이용한 application은 다음과 같다. 

- Deep Learning을 이용한 Vision 분야 application
  - Image Classification
  - Object Localization
  - Object Detection
  - Semantic Segmentation
  - Instance Segmentation

CS231n에서 말하지 않은 application은 다음과 같다. 
- 그 이외의 application
  - face recognition
  - object tracking
  - optical character recognition
  - pose estimation
  - image generation
  - style transfer
  - inpainting

Image classification은 vision 분야에서 가장 간단한 application이다. 이미지가 입력으로 들어오면 Neural Network는 이미지가 어떤 class에 해당하는지를 예측하는 것이다. 다음 그림은 이미지를 분류하는 과정을 보여준다. Image classification은 상당히 간단한 task이지만 AlexNet, GoogleNet, VGG, ResNet, DenseNet, MobileNet 등 모두 이 task를 풀기 위해 개발된 모델이다. 이 모델들은 image classification 이외의 다른 application 혹은 아예 다른 분야에서도 모델 구조로 사용되는 경우가 많다.  

<img src="https://www.dropbox.com/s/s9acumwosy14ah9/Screenshot%202018-11-04%2012.26.14.png?dl=1" width='500px'>
<center>이미지 출처: http://cs231n.stanford.edu/</center>

<br/>

상대적으로 연구를 하기에 간단한 task이기 때문에 image classification은 정말 다양한 연구가 되어있다. 단순히 모델 구조 뿐만 아니라 optimizer, weight initialization, regularization, data augmentation 등에 대해 풍부한 연구 결과가 쌓여있다. 따라서 image classification은 딥러닝을 처음 공부할 때 시작하기 좋은 분야이다. 


<br/>

### 더 좋은 모델 만들기
Image classification은 단순히 분류 문제이기 때문에 모델을 평가하기 쉽다. 테스트 데이터에서 대해서 얼마나 class를 잘 예측했는지를 통해 모델의 성능을 평가할 수 있다. 물론 계산량이나 모델 크기와 같은 수치로도 모델의 성능을 평가하기도 한다. 하지만 이 글에서는 accuracy만을 모델의 성능이라고 가정하고 이야기를 진행할 것이다. 딥러닝 학계에서는 모델의 성능을 올리기 위해 "딥"이라는 단어가 말해주듯 '어떻게 하면 모델을 더 깊게 만들 수 있을까'라는 주제에 노력을 기울였다. 하지만 2014년 VGG 논문에서 16 층의 딥러닝 모델을 만들고 2015년 GoogLeNet에서 22층의 딥러닝 모델을 만들면서 단순히 층을 쌓는 것의 한계를 체감한다. 하지만 GoogLeNet으로부터 VGG처럼 단순히 층을 쌓지 않고 창의적으로 구조를 변형하는 것이 효과가 있다는 것을 알게 된다. 2015년에 등장한 Batch Normalization과 함께 ResNet은 Paradigm Shift를 가져왔다.

ResNet을 포함해서 그 이후의 논문이 CIFAR-10에서 딥러닝 모델의 성능을 키우는 방법은 크게 다음 세가지로 구분할 수 있다. ResNet이 나온 이후에는 주로 ResNet의 구조를 다양하게 변형시켜가며 학습을 해보는 것에 집중했다. 하지만 딥러닝 모델이 training data에서 100%에 가까운 정확도를 달성하면서 초점이 조금 더 generalization 성능 향상에 맞춰졌다. 따라서 regularization에 관한 연구가 활발히 진행되었다. Generalization 성능을 높이기 위해서 regularization 말고도 data 자체를 다양하게 만드는 방법도 있다. 특히 CIFAR-10는 데이터수가 별로 없기 때문에 data augmentation 방법도 효과가 꽤나 많다. 그리고 사람이 경험적으로 네트워트 구조를 디자인하는 것을 neural network가 대신하는 Neural Architecture Search에 대한 연구가 최근 활발하다. 이런 흐름으로 CIFAR-10 정복하기 시리즈를 진행할 것이다. 

- 1 Network Architecture
  - ResNet
  - PyramidNet

- 2 regularization
  - Shake-shake regularization
  - Shake-drop regularization

- 3 Automatic Search Network
  - ENAS
  - Auto-Augment


<br/>

### 왜 CIFAR-10인가 
Image classification은 간단하면서도 앞으로 딥러닝을 공부하고 연구하는데 있어서 기본을 만들어주는 분야이다. Image classification에도 여러가지 dataset 있다. 그 중에서 CIFAR-10 데이터를 선택하였다. CIFAR-10 데이터는 학습데이터 50000개 테스트 데이터 10000개로 구성되어있다. 상당히 작은 데이터셋임에도 불구하고 가장 최근까지 정복이 안된 Challenge한 데이터셋이다. Auto-Augment 논문[^4]에서 1.48%의 error rate를 달성했다. 따라서 CIFAR-10 데이터셋은 최근에 정복되었다고 볼 수 있다. 지금까지 CIFAR-10에 적용되었던 논문을 살펴보면 어떻게 정복을 해나갔는지 그 흐름을 볼 수 있는 장점이 있다.

또한 많은 경우 딥러닝을 공부할 때는 개인 장비나 클라우드의 GPU를 활용하는 경우가 많다. CIFAR-10 데이터셋은 하나의 GPU만 가지고도 학습할 수 있기 때문에 실습하기가 쉽다. 

<br/>

### 이 post를 통해 기대하는 것
CIFAR-10 정복하기 시리즈를 끝내고 나면 다음이 가능하기를 기대한다. 

- 비전 분야 딥러닝 논문을 더 쉽고 빠르게 읽을 수 있다. 
- 딥러닝이 어떻게 발전해왔는지 그 흐름을 파악할 수 있다.
- 여러가지 application에서 딥러닝을 쓸 때 이해를 하고 쓸 수 있다. 

[^1]: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf
[^2]: https://blog.openai.com/learning-dexterity/
[^3]: http://cs231n.stanford.edu/
[^4]: https://arxiv.org/pdf/1805.09501.pdf
